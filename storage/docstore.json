{"docstore/metadata": {"252572b0-4d98-4297-adf5-e903d3caab61": {"doc_hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7"}, "df91900d-c71f-4c87-bcc4-da9d367b0869": {"doc_hash": "d8beff4a336ff793bb6712313724376e4bbc474374b156bdf65e6d52d04a4e9c", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "37300d2a-505a-4ef9-8893-62467eb29d69": {"doc_hash": "690b25517796f52ed2d62635d54636501256c3cd29705e743a42a84c903b64c9", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "af4915f1-bada-411c-9181-9ba72ada4879": {"doc_hash": "83136f118d6f10016bdcac048edcaecaeae0fa5136d347a4652834d10a7c3a84", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "c5159961-801d-4b39-8033-8ab03b9e19ed": {"doc_hash": "52cc7531800abd65f761ef478bb6285c54b33fbc7a2e2cb4b1c7a537e87b3839", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "be57c1d0-0221-4895-b221-6d155adf0abe": {"doc_hash": "aa48313796f019f34fbd5cefe9bbaaae3a00282b60b61d355a5d8f037e1c9a29", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "8d1b2091-f2e9-4ac5-8e9b-1fe1b7c3bb44": {"doc_hash": "4c21030265049e9568beab33b160df7965267c1a82ac854d6fb7673aacbcd0ac", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "9aeb9d5f-542b-4697-b814-d5506275ed3c": {"doc_hash": "64c9b9cf7cc865a599715c5939099892532798b17f4b919190b04feb4df6cfe1", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "df1575ec-5690-4190-bbb9-1adb89cdbe46": {"doc_hash": "832afd5f51219d22ca72c3d88658b9c020a33bfa3f21af8da4b2b94d1cd45a46", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "570f1ecd-88ad-472c-b0c5-f1890359ac45": {"doc_hash": "979e8c7a4e3984c850ad52a48cc4ff20c77deed01ba262fbcee4d6fa866723df", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "53c03067-c46c-4d89-a622-e31d357a40db": {"doc_hash": "ed2d447a06bfd42f78011fcfa0cc5897210774929b4a575bc030931121d99fce", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "2ef7482e-55a8-4960-b0de-62afa976c5ec": {"doc_hash": "80027a69b7aafb7c2605e83475d75133849e2f3ed3e9aa0daab13f3846077085", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "3658b62f-7129-4da9-8b34-07e460297fd2": {"doc_hash": "d2b3ffac57d5b5d99f6cbce2f89d28a55d78f0dd4c221d5c4af015dc02545c70", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "74cc0a89-667d-40b0-bb62-9e0f0a34af85": {"doc_hash": "6248df41fbec9c1cadc4071b6bf42608864744cc825e719314a660f8a67db183", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "def1c3ec-48fb-4786-81ac-23d724976d6a": {"doc_hash": "227f78726e8dba48dc73a1b5507c2ccfafac11c1a9e7b93c3458d3ecb9dd700a", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "ef4200eb-7a67-4f13-996f-212d08e9d221": {"doc_hash": "ba8478bdb0135d1fd546e92978d0b46947799a32d419a34a34f1733c2e5b6efd", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "8b5ffa73-eeaf-47b6-a29c-d572cdd7ea42": {"doc_hash": "4e589b6465b6a2172647c75a9055ed31841daeb925acb04f71b61d5f60c240b0", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "733503e8-6e1d-4351-8991-db2d630c56dd": {"doc_hash": "78b3f36f852a4cca2bf424ed63ccd90a0f3c03dff2038a8f0518a4ae18e5713a", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "4c431cfa-3853-44e9-bb21-1c1245c36752": {"doc_hash": "da7d5cdd85040d8d70a26c0fb06b3ed699dce639f48f1164eaf31ea4bccf50ca", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "6397dd93-c879-49f4-bf31-8d8576c3c3db": {"doc_hash": "7bb60e343b5689d9595c058839af19d5cb681e61b8cb310807ed1ba5f6723180", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "4b60359c-a455-4130-8861-a5fd25955fbd": {"doc_hash": "bb7cb05ed39c3d1f51e07f927351816c135e888fa2b45df993a89f2057470de5", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "407a718c-48b7-459d-a287-028f6f1899a8": {"doc_hash": "8dfee9e32b8d1b508466a0d188a0c88dee4f2acdd546546445502ad27d9c10a4", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "949714ca-56c3-48d7-ba3a-5337c63a519a": {"doc_hash": "da2df8e71b5e106190f9cf75dace1c071a264777cae7fa91d8af83909ef7d057", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "52e45947-6718-421f-bded-d83055e36d97": {"doc_hash": "fea2df02a4df0968413a1d4eda1bfb6e7a48ce0bb8d649e805e84d6ee1d8781f", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "2b93c396-2f42-4bb5-bff3-81fdbbe6546c": {"doc_hash": "8a283940f791ec78d7be38d01cbeb8ea0b2e9b26e3185c1255ea2484822160d2", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "b5fe8ee1-25b3-4515-8dc5-f4f73f5507ab": {"doc_hash": "d548bb474ed6dcea358b77a761a90a30a2db78d27b0cfb1a6e3c344dda646eb5", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "99c94d67-61af-42cc-8e2e-fae32d3ba3c6": {"doc_hash": "40e84ba2d4303f74fb358478523404bf56cb34e8aed90061a6fd48494092f925", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "e1ad92ac-3aeb-4719-9160-10c1dab00563": {"doc_hash": "4e854308e07ca7441e470f55d59c99c254522d7f9cf7f6020e671b80973c5818", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "9a9dbf37-a9b7-499d-9d97-815d3de50983": {"doc_hash": "b2ec50414138946e2a9cbd2d6e5ec91865a28c3300c0bf518f5ee2d3006883e6", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "5aa4b107-4757-47a2-a11e-9605c5f3da1f": {"doc_hash": "2355d37443c61d21fdb3153ac608b8e591a46941866e5874fd452b41555679f7", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "6faa1076-7c26-4e9e-a75c-a29c27698bbf": {"doc_hash": "460496fc53a4a5ef476a897005a60e51d37d6fa561bf140424726d912fcaeca9", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "7cb186a8-5a82-43af-a85a-3d7c9fb498cf": {"doc_hash": "3a3f1c5d5c446a293ef2e5cf24b1c45c6314d6c03d5cdf1a32d6d8231fd0dd8b", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "c6f75010-c249-4c13-b50b-072cee603031": {"doc_hash": "d47b57521d18884f710e35c69bac3d671d6a98d6c46a78b62d43f1436ec254a8", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "3dac663a-04cb-4da4-b383-37a12eb6d083": {"doc_hash": "8b5d21a3951902d63d094d9fc1f9b4fd51ecd0d6e831eed019668e74040765b8", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}, "7422bbd9-800e-4f59-8a85-667a600f69a2": {"doc_hash": "4f54bf3950cd71186e73b2173680b84f264e2efdb2a655420b632c4ed36eb067", "ref_doc_id": "252572b0-4d98-4297-adf5-e903d3caab61"}}, "docstore/data": {"df91900d-c71f-4c87-bcc4-da9d367b0869": {"__data__": {"id_": "df91900d-c71f-4c87-bcc4-da9d367b0869", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37300d2a-505a-4ef9-8893-62467eb29d69", "node_type": "1", "metadata": {}, "hash": "690b25517796f52ed2d62635d54636501256c3cd29705e743a42a84c903b64c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1   \nDIGITAL NOTES  \nON \nMachine  Learning  \n(R20D5803)  \nM.Tech., I I YEAR \u2013 I SEM  \n(2021 -2022)  \n \n \n \nDEPARTMENT  OF COMPUTER  SCIENCE  AND  \nENGINEERING  \n \nMALLA  REDDY  COLLEGE  OF ENGINEERING  & TECHNOLOGY  \n(Autonomous  Institution  \u2013 UGC,  Govt.  of India)  \n(Affiliated  to JNTUH,  Hyderabad,  Approved by  AICTE  - Accredited  by NBA  & NAAC \u2013 \u2018A\u2019 Grade  - ISO 9001:2015 Certified)  \nMaisammaguda,  Dhulapally  (Post  Via. Hakimpet),  Secunderabad  \u2013 500100,  Telangana  State,  INDIA.  \n2  \nMALLA REDDY  COLLEGE  OF ENGINEERING  & TECHNOLOGY  \nDEPARTMENT  OF COMPUTER  SCIENCE  AND  ENGINEERING  \nSYLLABUS   \nII Year  M. Tech.  CSE  \u2013 I Sem L/T/P/  C \n3 / - / -  3 \n(R20D5803)  Machine  Learning  \nObjectives:  \n1. This course  explains  machine  learning  techniques  such as decision  tree learning,  \nBayesian  learning etc.  \n2. To understand  computational  learning  theory.  \n3. To study  the pattern  comparison  techniques.  \n \nUNIT  - I \nIntroduction Well -posed learning problems, designing a learning  system  Perspectives  and issues  in \nmachine learning Concept learning and the general to specific ordering Introduction,A concept learning  \ntask, concept learning as search, Find -S: Finding a Maximally Specific Hypothes is, Version Spaces and  \nthe Candidate Elimination algorithm, Remarks on  Version Spaces and Candidate Elimination, Inductive  \nBias.  Decision  Tree Learning -Introduction, Decision Tree Representation, Appropriate Problems for  \nDecision Tree Learning, The Basic D ecision Tree Learning Algorithm Hypothesis Space  Search  in \nDecision  Tree Learning,  Inductive  Bias in Decision Tree  Learning,  Issues  in Decision  Tree Learning.  \n \nUNIT  - II \nArtificial Neural Networks -Introduction, Neural Network  Representation,  Appropriate Problems for  \nNeural  Network  Learning,  Perceptions,  Multilayer  Networks  and the Back  propagation  Algorithm.  \nDiscussion  on the Back  Propagation  Algorithm,  An illustrative Example:  Face  Recognition  \n \nUNIT  - III \nBayesian  learning -Introduction,  Byes  Theorem,  Bayes  Theorem  and Concept  Learning  Maximum  \nLikelihood  and Least  Squared  Error  Hypotheses,  Maximum  Likelihood  Hypotheses  for Predicting  \nProbabilities, Minimum Description Length Principle, Bayes Optimal Classifier, Gibs Algorithm, Na\u00efve  \nBayes  Classifier,  An Example:  Learning  to Classify  Text,  Bayesian Belief  Networks,  EM Algorithm.  \nInstance -Based  Learning -Introduction,  k-Nearest  Neighbor  Learning,  Locally  Weighted  Regression,  \nRadial  Basis  Functions,  Case -Based  Reasoning,  Remarks  on Lazy  and Eager  Learning.  \n \nUNIT  -IV \nPattern  Comparison  Techniques -Temporal  patterns,  Dynamic  Time  Warping  Methods,Clustering,  \nIntroduction  to clustering,  K-means  clustering,  K-Mode  Clustering.  Codebook  Generation,  Vector  \nQuantization.  \n \nUNIT  - V \nGenetic Algorithms: Different search m ethods for induction  - Explanation -based  Learning: using prior  \nknowledge  to reduce  sample  complexity.  Dimensionality  reduction:  feature  selection,  principal  \ncomponent  analysis,  linear  discriminate  analysis,  factor  analysis,  independent  component  analysis,  \nmultidimensional  scaling, and  manifold  learning.  3   \n \n \nTextbooks:  \n1. Machine  Learning  \u2013 Tom  M. Mitchell,  -MGH  \n2. Fundamentals  of Speech  Recognition  By Lawrence  Rabiner  and Biing  \u2013 Hwang  \nJuang .Ethem Alpaydin,  \u201dIntroduction to Machine Learning\u201d, MIT Press,  \nPrentice  Hall of India,  3 rd Edition2014.  \n3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37300d2a-505a-4ef9-8893-62467eb29d69": {"__data__": {"id_": "37300d2a-505a-4ef9-8893-62467eb29d69", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df91900d-c71f-4c87-bcc4-da9d367b0869", "node_type": "1", "metadata": {}, "hash": "d8beff4a336ff793bb6712313724376e4bbc474374b156bdf65e6d52d04a4e9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af4915f1-bada-411c-9181-9ba72ada4879", "node_type": "1", "metadata": {}, "hash": "83136f118d6f10016bdcac048edcaecaeae0fa5136d347a4652834d10a7c3a84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.  Mehryar  Mohri,  Afshin  Rostamizadeh,  Ameet  Talwalkar  \u201d Foundations  of Machine  \nLearning\u201d,MIT  Press,2012  \nReferences:  \n1. Machine  Learning  : An Algorithmic  Perspective,  Stephen  Marsland,  Taylor  & Francis  . 4   \n \nINDEX  \n \n \nS. No  \nUnit   \nTopic   \nPage  no \n \n1 I Introduction  Well -posed learning  problems  1 \n2 I A concept learning  task,  concept  learning as  search  6 \n3 I Find-S: Finding a  Maximally  Specific  Hypothesis  15 \n4 I Version Spaces and the Candidate Elimination  \nalgorithm  17 \n5 I Remarks  on Version  Spaces and  Candidate  \nElimination,  Inductive  Bias 21 \n6 I Decision  Tree Learning -Introduction,  Decision  Tree \nRepresentation  22 \n7 I Appropriate Problems for  Decision  Tree Learning  23 \n8 I Decision  Tree Learning  Algorithm,  Issues  in Decision  \nTree Learning.  25 \n \n \n \n \n \n \nS. No  \nUnit   \nTopic   \nPage  no \n \n1 II Artificial Neural Networks -Introduction,  \nNeural  Network  Representation  26 \n \n2 II  \nAppropriate Problems for Neural Network  \nLearning   \n28 \n3 II Perceptions, Multilayer Networks & the Back  \npropagation  Algorithm.  29 \n4 II Discussion  on the Back  Propagation  Algorithm  34 \nMALLA REDDY  COLLEGE  OF ENGINEERING  & TECHNOLOGY  \nDEPARTMENT  OF COMPUTER  SCIENCE  AND  ENGINEERING  5   \n \n \n \n \n \n \n \n \n \nS. No  \nUnit   \nTopic   \nPage  no \n1 III Bayesian learning -Introduction ,Bayes  \nTheorem &  Concept  Learning  maximum  36 \n2 III Maximum  Likelihood  Hypotheses  for Predicting  \nProbabilities(MAP)  42 \n3 III Gibs Algorithm,  Na\u00efve  Bayes  Classifier  46 \n4 III Minimum  Description  Length  Principle  , Bayes  \nOptimal  Classifier  47 \n \n5 III An Example:  Learning  to Classify  Text,  Bayesian  \nBelief  Networks   \n50 \n \n6 III EM Algorithm.  Instance -Based Learning -Introduction  51 \n7 III k-Nearest  Neighbor  Learning,  Locally  Weighted  \nRegression  55 \n8 III Radial  Basis  Functions,  Case -Based Reasoning  56 \n9 III Remarks  on Lazy  and Eager  Learning.  57 \nMALLA REDDY  COLLEGE  OF ENGINEERING  & TECHNOLOGY  \nDEPARTMENT  OF COMPUTER  SCIENCE  AND  ENGINEERING  6   \n \n \nS. No  \nUnit  Topic  Page  no \n1 IV Pattern  Comparison  Techniques -Temporal patterns,  58 \n \n2 IV  \nDynamic  Time  Warping Methods   \n61 \n3 IV Clustering  67 \n5 IV K-means  clustering  69 \n6 IV K-Mode  Clustering.  Codebook  Generation  70 \n7 IV Vector Quantization.  76 \n \n \n \n \n \n \n \nS. No  \nUnit   \nTopic   \nPage  no \n \n1 V Genetic Algorithms: Different search methods  \nfor induction   \n78 \n \n2 V  \nExplanation -based  Learning:  using  prior  knowledge  to \nreduce sample  complexity.   \n79 \n \n3 V Dimensionality  reduction   \n82 \n \n4 V  \nPrincipal  component analysis   \n84 \n \n5  \nV  \nLinear  discriminate  analysis,  factor  analysis,   \n85 \n \n6 V Independent  component  analysis:  multidimensional  \nscaling,  and manifold  learning.   \n86 \nMALLA REDDY  COLLEGE  OF ENGINEERING  & TECHNOLOGY  \nDEPARTMENT  OF COMPUTER  SCIENCE  AND  ENGINEERING  Department  of CSE MRCET  \n1  \n \n \n \nUNIT -I \n \n \nMachine  Learning  \nis the  field of study that gives  computers  the capability to learn without  \nbeing explicitly programmed. ML is one of the most exciting technologies  \nthat one would have ever  come  across. As it is  evident  from  the name,  it \ngives the computer that makes it more similar to humans: The ability to  \nlearn . Machine learning is actively being used today, perhaps in many more  \nplaces  than one would  expect.  \n \nMachine  Learning  is broadly  categorized  under  the following  headings:  \n \n \nMachine  learning  evolved  from  left to right  as shown  in the above  diagram.", "mimetype": "text/plain", "start_char_idx": 3529, "end_char_idx": 7132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af4915f1-bada-411c-9181-9ba72ada4879": {"__data__": {"id_": "af4915f1-bada-411c-9181-9ba72ada4879", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37300d2a-505a-4ef9-8893-62467eb29d69", "node_type": "1", "metadata": {}, "hash": "690b25517796f52ed2d62635d54636501256c3cd29705e743a42a84c903b64c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5159961-801d-4b39-8033-8ab03b9e19ed", "node_type": "1", "metadata": {}, "hash": "52cc7531800abd65f761ef478bb6285c54b33fbc7a2e2cb4b1c7a537e87b3839", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Initially,  researchers  started  out with Supervised  Learning.  This is the \ncase of housing  price  prediction  discussed earlier  \n. \u2022 This was followed  by unsupervised  learning,  where  the machine  is made  \nto learn  on its  own without  any supervision.  \n\u2022 Scientists discovered further that it may be a good idea to reward the  \nmachine  when  it does the job the expected  way and there  came  the \nReinforcem ent Learning.  \n\u2022 Very soon, the data that is available these days has become so humongous  \nthat the conventional techniques developed so far failed to analyse the big  \ndata and provide  us the predictions.  Department  of CSE MRCET  \n2  \n  \n \n \n \n \n\u2022 Thus, came the deep learning where the human brain is simulated in the  \nArtificial  Neural Networks (ANN)  created  in our binary computers.  \n\u2022 The machine now learns on its own using the high computing power and  \nhuge  memory  resources that  are available today.  \n\u2022 It is now observed that Deep Learning has solved many of the previously  \nunsolvable  problems.  \n\u2022 The technique is now further advanced by giving incentives to Deep  \nLearning networks as awards and there finally comes Deep Reinforcement  \nLearning.  \nLet us now study  each of these  categories  in more  details  \nSupervised  Learning:  \nSupervised  learning  is analogous  to training  a child  to walk.  You will hold \nthe child\u2019s hand, show him how to take his foot forward, walk yourself for a  \ndemonstration  and so  on, until the  child learns  to walk on his  own.  \nRegression:  \nSimilarly,  in the case of supervised  learning,  you give concrete  known  \nexamples to the computer. You say that for given feature value x1 the output  \nis y1, for x2 it is y2, for x3 it is y3, and so on. Based on this data, you let the  \ncomputer figure out an empirical relationship between x and y. Once the  \nmachine is trained in this way with a sufficient number of data points, now  \nyou would ask the machi ne to predict Y for a given X. Assuming that you  \nknow the real value of Y for this given X, you will be able to deduce whether  \nthe machine\u2019s prediction is correct. Thus, you will test whether the machine  \nhas learned by using the known test data. Once you are satisfied that the  \nmachine is able to do the predictions with a desired level of accuracy (say 80  \nto 90%) you can stop further training the machine. Now, you can safely use  \nthe machine  to do the predictions  on unknown  data points,  or ask the \nmachine to  predict Y for a given X for which you do not know the real value  \nof Y. This training comes  under  the regression  that we  talked about earlier.  Department  of CSE MRCET  \n3  \n  \nClassification:  \nYou may also use machine learning techniqu es for classification problems. In  \nclassification problems, you classify objects of similar nature into a single  \ngroup. For example, in a set of 100 students say, you may like to group them  \ninto three groups based on their heights - short, medium and long.  Measuring  \nthe height of each student, you will place them in a proper group. Now, when  \na new student  comes  in, you will put him in an appropriate  group  by \nmeasuring his height. By following the principles in regression training, you  \nwill train the machine  to classify a student based on his feature \u2013 the height.  \nWhen the machine learns how the groups are formed, it will be able to  \nclassify any unknown new student correctly. Once again, you would use the  \ntest data to verify  that the machine  has learned  your technique  of \nclassification before putting the developed model in production. Supervised  \nLearning  is where  the AI really  began  its journey.  This technique  was \napplied successfully in several cases. You have used this model while doing  \nthe hand -written reco gnition on your machine. Several algorithms have been  \ndeveloped  for supervised  learning.  You will learn  about  them  in the \nfollowing chapters.  \nUnsupervised  Learning:  \nIn unsupervised learning, we do not specify a target variable to the machine,  \nrather we ask  machine \u201cWhat can you tell me about X?\u201d.", "mimetype": "text/plain", "start_char_idx": 7135, "end_char_idx": 11252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5159961-801d-4b39-8033-8ab03b9e19ed": {"__data__": {"id_": "c5159961-801d-4b39-8033-8ab03b9e19ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af4915f1-bada-411c-9181-9ba72ada4879", "node_type": "1", "metadata": {}, "hash": "83136f118d6f10016bdcac048edcaecaeae0fa5136d347a4652834d10a7c3a84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be57c1d0-0221-4895-b221-6d155adf0abe", "node_type": "1", "metadata": {}, "hash": "aa48313796f019f34fbd5cefe9bbaaae3a00282b60b61d355a5d8f037e1c9a29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More specifically,  \nwe may ask questions  such as given  a huge  data set X, \u201cWhat  are the five \nbest groups we can make out of X?\u201d or \u201cWhat features occur together most  \nfrequently  in X?\u201d.  To arrive  at the answers  to such questions,  you can \nunderstand that the number of data points that the machine would require to  \ndeduce a strategy would be very large. In case of supervised learning, the  \nmachine  can be trained  with even  about  few thousands  of data points.  \nHowever, in case of unsupervised learning, the number of data points that is  \nreasonably accepted for learning starts in a few millions. These days, the data  \nis generally  abundantly  available.  The data ideally  requires  curating.  \nHowever, the amount of data that is cont inuously flowing in a social area  \nnetwork, in most cases data curation is an impossible task. The following  \nfigure shows the boundary between the yellow and red dots as determined by  \nunsupervised  machine  learning.  You  can  see it clearly  that  the machine  Department  of CSE MRCET  \n4  \n  \nwould be able to determine the class of each of the black dots with a fairly  \ngood accuracy.  \n \n \nReinforcement  Learning:  \nConsider training a pet dog, we train our pet to bring a ball to us. We throw  \nthe ball at a certain distance and ask the dog to fetch it back to us. Every time  \nthe dog does this right, we reward the dog. Slowly, the dog learns that doing  \nthe job rightly gives him a reward and then the dog starts doing the job right  \nway every time in future. Exactly, this concept is applied in \u201cReinforcement\u201d  \ntype of learning. The technique was initially developed for machines to play  \ngames. The machine is given an algorithm to analyse all possible moves at  \neach stage of the game. The machine may select one of the moves at random.  \nIf the move is right, the machine is rewarded, otherwise it may be penalized.  \nSlowly, the machine will start differentiating between right and wrong moves  \nand after several iterations would learn to solve the game puzzle with a better  \naccuracy. The accuracy of winning the game would improve as the machine  \nplays more and  more  games.  \nThe entire process  may be depicted  in the following  diagram:  \nDepartment  of CSE MRCET  \n5  \n  \n \n \n \nDeep  Learning:  \nThe deep learning is a model based on Artificial Neural Networks (ANN),  \nmore specifically Convolutional Neural Networks (CNN)s. There are several  \narchitectures used in deep learning such as deep neural n etworks, deep belief  \nnetworks,  recurrent  neural  networks,  and convolutional  neural  networks.  \nThese networks have been successfully applied in solving the problems of  \ncomputer  vision,  speech  recognition,  natural  language  processing,  \nbioinformatics, drug des ign, medical image analysis, and games. There are  \nseveral other fields in which deep learning is proactively applied. The deep  \nlearning  requires  huge  processing  power  and humongous  data,  which  is \ngenerally easily available these days. We will talk about deep learning more  \nin detail  in the coming  chapters.  \nDeep  Reinforcement  Learning : \nThe Deep Reinforcement Learning (DRL) combines the techniques of both  \ndeep and reinforcement learning. The reinforcement learning algorithms like  \nQ learning are now combined with deep learning to create a powerful DRL  \nmodel. The technique has been with a great success in the fields of robotics,  \nvideo games, finance and healthcare. Many previously unsolvable problems  \nare now solved  by creating  DRL  models.  There  is lots of resea rch going  on \nin this area and this is very actively  pursued  by the industries.  So far, you \nDepartment  of CSE MRCET  \n6  \n  \nhave got a brief introduction to various machine learning models, now let us  \nexplore slightly deeper into various algorithms that are available under these  \nmodels.  \nWell  posed learning  problems:  \n \nA computer program is said to learn from experience E in context to some  \ntask T and some performance measure P, if its performance on T, as was  \nmeasured by  P, upgrades  with experience  E. \nAny problem can be segregated as well -posed learning problem if it has three  \ntraits \u2013 \n\u2022 Task  \n\u2022 Performance  Measure  \n\u2022 Experience  \nCertain  example  that efficiently  defines  the well-posed  learning  problems  \nare: \n \n1.", "mimetype": "text/plain", "start_char_idx": 11253, "end_char_idx": 15604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be57c1d0-0221-4895-b221-6d155adf0abe": {"__data__": {"id_": "be57c1d0-0221-4895-b221-6d155adf0abe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5159961-801d-4b39-8033-8ab03b9e19ed", "node_type": "1", "metadata": {}, "hash": "52cc7531800abd65f761ef478bb6285c54b33fbc7a2e2cb4b1c7a537e87b3839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d1b2091-f2e9-4ac5-8e9b-1fe1b7c3bb44", "node_type": "1", "metadata": {}, "hash": "4c21030265049e9568beab33b160df7965267c1a82ac854d6fb7673aacbcd0ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To better  filter  emails  as spam  or not \n\u2022 Task  \u2013 Classifying  emails  as spam  or not \n\u2022 Performance  Measure  \u2013 The fraction  of emails  accurately  classified  as spam  \nor not spam  \n\u2022 Experience  \u2013 Observing  you label  emails  as spam  or not spam  \n2. A checkers  learning  problem  \n\u2022 Task  \u2013 Playing  checkers  game  \n\u2022 Performance  Measure  \u2013 percent  of games  won against  opposer  \n\u2022 Experience  \u2013 playing  implementation  games  against  itself  \n3. Handwriting  Recognition  Problem  \n\u2022 Task  \u2013 Acknowledging  handwritten  words  within  portrayal  \n\u2022 Performance  Measure  \u2013 percent  of words  accurately  classified  \n\u2022 Experience  \u2013 a directory  of handwritten  words  with given  classifications  \n4. A Robot  Driving  Problem  \n\u2022 Task  \u2013 driving  on public  four-lane highways  using  sight  scanners  \n\u2022 Performance  Measure  \u2013 average  distance  progressed  before  a fallacy  \n\u2022 Experience  \u2013 order  of images  and steering  instructions noted down  while  \nobserving a  human  driver  \n5. Fruit  Prediction  Problem  Department  of CSE MRCET  \n7  \n  \n\u2022 Task  \u2013 forecasting  different  fruits  for recognition  \n\u2022 Performance  Measure  \u2013 able to predict  maximum  variety  of fruits  \n\u2022 Experience  \u2013 training  machine  with the largest  datasets  of fruits  images  \n \n6. Face  Recognition  Problem  \n\u2022 Task  \u2013 predicting  different  types  of faces  \n\u2022 Performance  Measure  \u2013 able to predict  maximum  types  of faces  \n\u2022 Experience  \u2013 training  machine  with maximum  amount  of datasets  of \ndifferent face images  \n7. Automatic  Translation  of documents  \n\u2022 Task  \u2013 translating  one type of language  used in a document  to other  language  \n\u2022 Performance  Measure  \u2013 able to convert  one language  to other  efficiently  \n\u2022 Experience  \u2013 training  machine  with a large  dataset  of different  types  of \nlanguages  \n \nDesign  of a learning  system:  \n \nJust now we looked into the learning process and also understood the goal  \nof the learning.  When  we want  to design  a learning  system  that follows  the \nlearning process, we need to consider a few design choices. The design  \nchoices  will be to decide  the following key  components:  \n1. Type  of training  experience  \n2. Choosing  the Target  Function  \n3. Choosing  a representation  for the Target  Function  \n4. Choosing  an approximation  algorithm  for the Target  Function  \n5. The final Design  \nWe will look into the game  - checkers  learning  problem  and apply  the above  \ndesign  choices.  For a checkers  learning  problem,  the three  elements  will be, \n\u2022 Task  T: To play checkers  \n\u2022 Performance  measure  P: Total  present  of the game  won in the tournament.  \n\u2022 Training  experience  E: A set of games  played  against  itself.  \n \nType  of training  experience:  \nDuring the  design  of the checker's  learning  system, the type of  training  \nexperience  available  for a learning  system  will have  a significant  effect  on \nthe success  or failure  of the  learning.  Department  of CSE MRCET  \n8  \n  \nDirect  or Indirect  training  experience:  \nIn the case of direct  training  experience,  an individual  board  states  and \ncorrect move for each board state are given. In case of indirect training  \nexperience, the move sequences for a game and the final result (win, lose or  \ndraw) are given for a number of games. How to assign credit or blame to  \nindividual moves is  the credi t assignment  problem.  \n \n1. Teacher  or Not: \n\uf0a2 Supervised:  \nThe training experience will be labelled, which means, all the board states  \nwill be labelled with the correct move. So the learning takes place in the  \npresence  of a supervisor  or a teacher.  \n\uf0a2 Un-Supervised:  \nThe training  experience  will be unlabelled,  which  means,  all the board  \nstates will not have the moves. So the learner generates random games and  \nplays against itself  with no supervision  or teacher  involvement.", "mimetype": "text/plain", "start_char_idx": 15605, "end_char_idx": 19521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d1b2091-f2e9-4ac5-8e9b-1fe1b7c3bb44": {"__data__": {"id_": "8d1b2091-f2e9-4ac5-8e9b-1fe1b7c3bb44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be57c1d0-0221-4895-b221-6d155adf0abe", "node_type": "1", "metadata": {}, "hash": "aa48313796f019f34fbd5cefe9bbaaae3a00282b60b61d355a5d8f037e1c9a29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aeb9d5f-542b-4697-b814-d5506275ed3c", "node_type": "1", "metadata": {}, "hash": "64c9b9cf7cc865a599715c5939099892532798b17f4b919190b04feb4df6cfe1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\uf0a2 Semi -supervised:  \nLearner  generates  game  states  and asks the teacher  for help in finding  \nthe correct  move if the  board  state is confusing.  \n2. Is the training  experience  good:  \n \n\uf0a2 Do the  training  examples  represent  the distribution of  examples over  \nwhich the final system performance will be measured? Pe rformance is best  \nwhen  training  examples  and test examples  are from  the same/a  similar  \ndistribution.  \n\uf0a2 The checker player learns by playing against oneself. Its experience is  \nindirect. It may not encounter moves that are common in human expert play.  \nOnce the  proper training experience is available, the next design step will be  \nchoosing the  Target  Function.  \nChoosing  the Target  Function:  \nWhen  you are playing  the checkers  game,  at any moment  of time,  you make  \na decision on choosing the best move from different possibilities. You think  \nand apply the learning that you have gained from the experience. Here the  \nlearning  is, for a specific  board,  you move  a checker  such that your board  Department  of CSE MRCET  \n9  \n \n \nstate tends towards the winning situation. Now the same learning has to be  \ndefined  in terms  of the target function.  \nHere  there  are 2 considerations \u2014 direct  and indirect  experience.  \n\u2022 During the direct experience the checkers learning system, it needs only  \nto learn how to choose the best move among some large search space. We  \nneed to find a target function that will help us choose the best move among  \nalternatives.  \nLet us call this function  Choose  Move  and use the notation  Choose  Move:  B \n\u2192M to indicate that this function accepts as  input any board from the set of  \nlegal board states B and produces as output some move from the set of legal  \nmoves  M. \n\u2022 When there is an indirect experience it becomes difficult to learn such  \nfunction.  How about  assigning a  real score  to the  board state.  \n \n \nSo the function be V: B \u2192R indicating that this accepts as input any board  \nfrom the set of legal board states B and produces an output a real score. This  \nfunction assigns  the higher  scores to  better  board  states  \n \n \n \n \nIf the system  can successfully  learn  such a target  function  V, then it can \neasily use  it to select  the best move from  any board position.  \nLet us therefore define the target value V(b) for an arbitrary board state b in  \nB, as follows:  Department  of CSE MRCET  \n10  \n  \n1. if b is a final board  state that is won,  then V(b)  = 100 \n2. if b is a final board  state that is lost,  then V(b)  = -100 \n3. if b is a final board  state that is drawn,  then V(b)  = 0 \n4. if b is a not a final state in the game, then V (b) = V (b\u2019), where b\u2019 is the best  \nfinal board state that can be achieved starting from b and playing optimally  \nuntil the end  of the game.  \nThe (4) is a recursive definition and to determine the value of V(b) for a  \nparticular board state, it performs the search ahead for the optimal line of  \nplay,  all the way to the end of the game. So this definition is not efficiently  \ncomputable  by our checkers  playing  program,  we say that it is a non- \noperational  definition.  \n \n \nChoosing  a representation  for the Target  Function:  \nNow that we have specified the ideal target function V, we must choose a  \nrepresentation that the learning program will use to describe the function ^V  \nthat it will learn. As with earlier design choices, we again have many options.  \nWe could, for example, allow the program to represent us ing a large table  \nwith a distinct entry specifying the value for each distinct board state. Or we  \ncould allow it to represent using a collection of rules that match against  \nfeatures of the board state, or a quadratic polynomial function of predefined  \nboard  features, or an artificial neural network. In general, this choice of  \nrepresentation involves a crucial trade off.", "mimetype": "text/plain", "start_char_idx": 19526, "end_char_idx": 23442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9aeb9d5f-542b-4697-b814-d5506275ed3c": {"__data__": {"id_": "9aeb9d5f-542b-4697-b814-d5506275ed3c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d1b2091-f2e9-4ac5-8e9b-1fe1b7c3bb44", "node_type": "1", "metadata": {}, "hash": "4c21030265049e9568beab33b160df7965267c1a82ac854d6fb7673aacbcd0ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df1575ec-5690-4190-bbb9-1adb89cdbe46", "node_type": "1", "metadata": {}, "hash": "832afd5f51219d22ca72c3d88658b9c020a33bfa3f21af8da4b2b94d1cd45a46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In general, this choice of  \nrepresentation involves a crucial trade off. On one hand, we wish to pick a  \nvery expressive  representation  to allow  representing  as close  an \napproximation as  possible to the ideal  target function  V. \nOn the other hand, the more expressive the representation, the more training  \ndata the program  will require  in order  to choose  among  the alternative  \nhypotheses it can represent. To keep the discussion brief, let us choose a  \nsimple represe ntation:  for any given board state, the function ^V will be  \ncalculated  as a linear  combination of  the following  board features:  \n\u2022 x1(b)  \u2014 number  of black pieces  on board  b \n\u2022 x2(b)  \u2014 number of  red pieces  on b \n\u2022 x3(b)  \u2014 number  of black kings on  b Department  of CSE MRCET  \n11  \n  \n\u2022 x4(b)  \u2014 number  of red kings on  b \n\u2022 x5(b) \u2014 number of red pieces threatened by black  \u2022 x6(b) \u2014 number of  \nblack  pieces  threatened  by red \n^V = w0 + w1 \u00b7 x1(b) +  w2 \u00b7 x2(b) + w3 \u00b7 x3(b)  + w4 \u00b7  x4(b) +w5 \u00b7 x5(b) + w6 \u00b7  x6(b)  \n \n \nWhere w0 through w6 are  numerical  coefficients or  weights to be obtained  \nby a learning  algorithm.  Weights  w1 to w6 will determine  the relative  \nimportance  of different  board  features.  \nSpecification of the Machine Learning Problem at thi s time: Till now we  \nworked on choosing the type of training experience, choosing the target  \nfunction  and its representation.  The checkers  learning  task can be \nsummarized as  below.  \n\u2022 Task  T: Play Checkers  \n\u2022 Performance  Measure:  % of games  won in world  tournament  \n\u2022 Training  Experience  E: opportunity  to play against  itself  \n\u2022 Target  Function:  V: Board  \u2192 R \n\u2022 Target Function Representation: ^V = w0 + w1 \u00b7 x1(b) + w2 \u00b7 x2(b) + w3 \u00b7  \nx3(b)  + w4 \u00b7 x4(b) +w5  \u00b7 x5(b)  + w6 \u00b7 x6(b)  \nThe first three items above correspond to the specification of the learning  \ntask,  where  as the final two items  constitute  design  choices  for the \nimplementation of  the learning  program.  \n \n \n \n \nChoosing  an approximation  algorithm  for the Target  Function:  \nGenerating training data \u2014 To train our learning program, we need a set of  \ntraining data, each describing a specific board state b and the training value  \nV_train  (b) for b. Each training  example  is an ordered  pair <b,v_train(b)>.  Department  of CSE MRCET  \n12  \n \n \nTemporal  difference  (TD)  learning  is a concept  central  to reinforcement  \nlearning, in which learning happens through the iterative correction of your  \nestimated returns towards  a more accurate  target return.  \n\uf056 V_train(b)  \u2190 ^V(Successor(b))  \n \nFinal  Design  for Checkers  Learning  system:  \nThe final design of our checkers learning system can be naturally described  \nby four distinct program modules that represent the central components in  \nmany learning  systems.  \n1. The performance System: Takes a new board as input and outputs a trace of  \nthe game it  played  against  itself.  \n2. The Critic: Takes the trace of a game as an input and outputs a set of training  \nexamples  of the  target  function.  \n3. The Generalizer: Takes training examples as input and outputs a hypothesis  \nthat estimates  the target  function.  Good  generalization  to new cases  is \ncrucial.  \n4. The Experiment Generator: Takes the current hypothesis (currently learned  \nfunction) as input and outputs a new problem (an initial board state) for the  \nperformance  system  to explore.  \n \n \n \nIssue s in Machine Learning:  Department  of CSE MRCET  \n13  \n  \nOur checkers example raises a number of generic questions about machine  \nlearning. The field of machine learning, and much of this book, is concerned  \nwith answering  questions such  as the following:  \n\u2022 What algorithms exist for learning general target functions from specific  \ntraining examples?", "mimetype": "text/plain", "start_char_idx": 23369, "end_char_idx": 27177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df1575ec-5690-4190-bbb9-1adb89cdbe46": {"__data__": {"id_": "df1575ec-5690-4190-bbb9-1adb89cdbe46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9aeb9d5f-542b-4697-b814-d5506275ed3c", "node_type": "1", "metadata": {}, "hash": "64c9b9cf7cc865a599715c5939099892532798b17f4b919190b04feb4df6cfe1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "570f1ecd-88ad-472c-b0c5-f1890359ac45", "node_type": "1", "metadata": {}, "hash": "979e8c7a4e3984c850ad52a48cc4ff20c77deed01ba262fbcee4d6fa866723df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In what settings will particular algorithms converge to the  \ndesired function, given sufficient training data? Which algorithms perform  \nbest for  which  types  of problems  and representations?  \n\u2022 How much training data is sufficient? What general bounds can be found to  \nrelate  the confidence  in learned  hypotheses  to the amount  of training  \nexperience  and the character  of the learner's hypothesis  space?  \n\u2022 When and how can prior knowledge held by the learner guide the process of  \ngeneralizing from examples? Can prior knowledge be helpful even when it is  \nonly approximately  correct?  \n\u2022 What is the best strategy for choosing a useful next training experience, and  \nhow does the choice of this strategy alter the complexity of the learning  \nproblem?  \n\u2022 What is the best way to reduce the learning task to one or more function  \napproximation  problems?  Put another  way,  what  specific  functions  should  \nthe system  attempt  to learn?  Can this process  itself  be automated?  \n\u2022 How can the learner automatically alter its representation to improve its  \nability to  represent and  learn  the target function?  \nCONCEPT  LEARNING:  \n \n\u2022 Inducing general functions from specific training examples is a main issu e of \nmachine  learning.  \n\u2022 Concept  Learning : Acquiring  the definition  of a general category  from  \ngiven  sample  positive  and negative  training  examples  of the category.  \n\u2022 Concept Learning can see as a problem of searching through a predefined  \nspace of potential hypotheses for the hypothesis that best fits the training  \nexamples.  \n\u2022 The hypothesis space has a general -to-specific ordering of hypotheses, and  \nthe search can be efficiently organized by taking advantage of a naturally  \noccurring structure over  the hypothesi s space.  \nA Formal  Definition  for Concept  Learning:  Department  of CSE MRCET  \n14  \n \n \nInferring a Boolean -valued function from training examples of its input and  \noutput.  \n\u2022 An example for concept -learning is the learning of bird -concept from the  \ngiven  examples  of birds  (positive  examples)  and non-birds  (negative  \nexamples).  \n\u2022 We are trying to learn the definition of a concept from given examples.  \nA Concept Learning  Task: Enjoy Sport Training  Examples  \n \n \nA set of example days, and each is described by six attributes.  The task is to  \nlearn  to predict  the value  of Enjoy  Sport  for arbitrary  day, based  on the \nvalues of its  attribute  values.  \n \n \nConcept  Learning  as Search:  \n\u2022 Concept  learning  can be viewed  as the task  of searching  through  a large  \nspace  of hypotheses  implicitly  defined  by the hypothesis  representation.  \n\u2022 The goal of this search is to find the hypothesis that best fits the training  \nexamples.  \n\u2022 By selecting  a hypothesis  representation,  the designer  of the learning  \nalgorithm implicitly def ines the space of all hypotheses that the program can  \never represent  and therefore  can ever learn.  Department  of CSE MRCET  \n15  \n  \nFIND -S: \n\u2022 FIND -S Algorithm starts from the most specific hypothesis and generalize it  \nby considering  only positive  examples.  \n\u2022 FIND -S algorithm  ignores  negative  example  \n: As long as the hypothesis space contains a hypothesis that  describes the  \ntrue target  concept,  and the training  data contains  no errors,  ignoring  \nnegative  examples  does not  cause  to any  problem.  \n\u2022 FIND -S algorithm  finds  the most  specific  hypothesis  within  H that is \nconsistent with the positive training examples. \u2013 The final hypothesis will  \nalso be consistent with negative examples if  the correct target concept is in  \nH, and the training  examples are  correct.  \nFIND -S Algorithm:  \n1. Initialize  h to the most  specific  hypothesis  in H \n2. For each positive  training  instance  x For each  attribute  \nconstraint a,  in h \nIf the constraint a, is satisfied by x  \nThen do  nothing  \n3. Else replace  a, in h by the next more  general  constraint  that is satisfied  by \nx 4. Output  hypothesis  h \nFIND -S Algorithm  \u2013 Example:  \nImportant -Representation:  \n \n1. ? indicates  that any value  is acceptable  for the attribute.  \n2. specify  a single  required  value  (e.g.,  Cold)  for the attribute.  \n3. \u03a6 indicates  that no value  is acceptable.  \n4.", "mimetype": "text/plain", "start_char_idx": 27178, "end_char_idx": 31472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "570f1ecd-88ad-472c-b0c5-f1890359ac45": {"__data__": {"id_": "570f1ecd-88ad-472c-b0c5-f1890359ac45", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df1575ec-5690-4190-bbb9-1adb89cdbe46", "node_type": "1", "metadata": {}, "hash": "832afd5f51219d22ca72c3d88658b9c020a33bfa3f21af8da4b2b94d1cd45a46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53c03067-c46c-4d89-a622-e31d357a40db", "node_type": "1", "metadata": {}, "hash": "ed2d447a06bfd42f78011fcfa0cc5897210774929b4a575bc030931121d99fce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. \u03a6 indicates  that no value  is acceptable.  \n4. The most  general  hypothesis is represented  by: {?, ?, ?, ?, ?, ?} \n5. The most  specific  hypothesis is represented  by: {\u03d5, \u03d5, \u03d5, \u03d5, \u03d5, \u03d5} \n \nSteps  Involved in  Find -S: \n1. Start  with the  most  specific  hypothesis.  h = {\u03d5, \u03d5, \u03d5, \u03d5, \u03d5, \u03d5} Department  of CSE MRCET  \n16  \n \n \n2. Take  the next example  and if it is negative,  then no changes  occur  to the \nhypothesis.  \n3. If the example  is positive  and we find that our initial  hypothesis  is too \nspecific  then we update  our current hypothesis to a  general condition.  \n4. Keep  repeating  the above  steps  till all the training  examples  are complete.  \n5. After we have completed all the training examples we will have the final  \nhypothesis when can use to classify the new examples. Example: Consider  \nthe following  data set having  the data about  which  particular  seeds  are \npoisonous.  \n \n \n \nFirst, we consider the hypothesis to be a more specific hypothesis. Hence,  \nour hypothesis  would  be: h = {\u03d5, \u03d5,  \u03d5, \u03d5, \u03d5, \u03d5} \n \n \nConsider  example  1: \nThe data in example 1 is {GREEN, HARD, NO, WRINKLED}. We see that  \nour initial hypothesis is more specific and we have to generalize it for this  \nexample.  \nHence,  the hypothesis  becomes:  \nh = {GREEN,  HARD,  NO, WRINKLED}  \nConsider  example  2: Department  of CSE MRCET  \n17  \n  \nHere we see that this example has a negative outcome. Hence we neglect  \nthis example  and our hypothesis  remains  the same.  h = {GREEN,  \nHARD,  NO, WRINKLED}  \nConsider  example  3: \nHere we see that this example has a negative outcome. hence we neglect  \nthis example  and our hypothesis  remains  the same.  h = {GREEN,  \nHARD,  NO, WRINKLED}  \nConsider  example  4: \nThe data present  in example  4 is {ORANGE,  HARD,  NO, WRINKLED}.  \nWe \ncompare every single attribute with the  initial data and if any mismatch is  \nfound we replace that particular attribute with a general case (\u201c ?\u201d). After  \ndoing  the process  the hypothesis  becomes:  h = {?, HARD,  NO, \nWRINKLED }  \nConsider  example  5: \nThe data present in example 5 is {GREEN, SOFT, YES,  SMOOTH}. We  \ncompare every single attribute with the initial data and if any mismatch is  \nfound we replace that particular attribute with a general case ( \u201c?\u201d ). After  \ndoing  the process  the hypothesis  becomes:  \nh = {?,  ?, ?, ? } \nSince we have reached a point  where all the attributes in our hypothesis  \nhave the general condition, example 6 and example 7 would result in the  \nsame  hypothesizes  with all general  attributes.  h = {?, ?, ?, ? } \nHence,  for the given  data the final hypothesis  would  be: \nFinal  Hypothesis:  h = { ?, ?, ?, ? }. \n \nVersion  Spaces  \nDefinition(Version space). A concept is complete if it covers all positive  \nexamples.  \nA concept is consistent if it covers none of the negative examples. The  \nversion space is the set of all complete and consistent concepts. This set is  \nconvex and  is fully defined  by its least and  most  general elements.  \nCandidate -Elimination  Learning  Algorithm  Department  of CSE MRCET  \n18  \n \n \nThe CANDIDATE -ELIMINTION algorithm computes the version space  \ncontaining  all hypotheses  from  H that are consistent  with an observed  \nsequence  of training  examples.", "mimetype": "text/plain", "start_char_idx": 31422, "end_char_idx": 34705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53c03067-c46c-4d89-a622-e31d357a40db": {"__data__": {"id_": "53c03067-c46c-4d89-a622-e31d357a40db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "570f1ecd-88ad-472c-b0c5-f1890359ac45", "node_type": "1", "metadata": {}, "hash": "979e8c7a4e3984c850ad52a48cc4ff20c77deed01ba262fbcee4d6fa866723df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ef7482e-55a8-4960-b0de-62afa976c5ec", "node_type": "1", "metadata": {}, "hash": "80027a69b7aafb7c2605e83475d75133849e2f3ed3e9aa0daab13f3846077085", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Initialize  G to the set of maximally  general  hypotheses  in H Initialize  S to \nthe set of maximally  specific  hypotheses  in H For each training  example  d, \ndo \n\u2022 If d is a positive  example  \n\u2022 Remove  from  G any hypothesis  inconsistent  with d \n\u2022 For each hypothesis  s in S that is not consistent  with d \n\u2022 Remove s  from  S \u2022 Add to S all minimal  generalizations  h of s such that h is \nconsistent  with d, and some  member  of G is more  general  than h \n\u2022 Remove  from  S any hypothesis  that is more  general  than another  hypothesis  \nin S \n\u2022 If d is a negative  example  \n\u2022 Remove  from  S any hypothesis  inconsistent  with d \n\u2022 For each hypothesis  g in G that is not consistent  with d \n\u2022 Remove  g from  G 18\\ \n\u2022 Add to G all minimal  specializations h  of g such that  \n\u2022 h is consistent with  d, and some  member  of S is more  specific  than h \n\u2022 Remove  from  G any hypothesis  that is less general  than another  hypothesis  \nin G. \nCANDIDATE - ELIMINTION algorithm using version spaces An \nIllustrative  Example:  \n Department  of CSE MRCET  \n19  \n  \nCANDIDATE -ELIMINTION  algorithm  begins  by initializing  the version  \nspace  to the set of all hypotheses  in H; \nboundary set to contain the most general hypothesis in H,  G0 ?, ?, ?, ?, ?,  \nWhen  the first training example is presented,  the \nCANDIDATEELIMINTION  algorithm  checks  the S boundary  and finds  that \nit is overly specific  and it fails to cover  the positive  example.  \n\u2022 The boundary  is therefore  revised  by moving  it to the least more  general  \nhypothesis that  covers this  new example.  \n\u2022 No update  of the G boundary  is needed  in response  to this training  example  \nbecause  Go correctly  covers  this example.  \n \n \n \n \n \n \n \n \n \n \n \n\u2022 When  the second  training  example  is observed,  it has a similar  effect  of \ngeneralizing  S further  to S2,  leaving  G again unchanged  i.e., G2 = G1 =G0 \nDepartment  of CSE MRCET  \n20  \n \n \n\u2022 Consider the third training example. This negative example reveals that the  \nboundary of the version space is overly general, that is, the hypothesis in G  \nincorrectly predicts  that this new example  is a positive  example.  \n\u2022 The hypothesis in the G boundary  must  therefore be specialized  until it  \ncorrectly classifies  this new negative  example.  \n \n \n \n \nGiven that there are six attributes that could be specified to  specialize G2,  \nwhy are  there  only three new  hypotheses  in G3?  \n \nFor example,  the hypothesis  h = (?, ?, Normal,  ?, ?, ?) is a minimal  \nspecialization of G2 that correctly labels the new example as a negative  \nexample, but it is not included in G3. The reason this hypothesis is excluded  \nis that it is inconsistent with the previously encountered positive examples.  \nConsider  the fourth  training example.  Department  of CSE MRCET  \n21  \n  \n \n \n\u2022 This positive example  further generalizes  the S boundary of  the version  \nspace. It also results in removing one member of the  G boundary, because  \nthis member fails to cover the new positive example After processing these  \nfour examples, the boundary sets S4 and G4 delimit the version space of all  \nhypotheses  consistent  with the set of incrementally  observed  training  \nexamples.  \n\u2022 After processing these four examples, the boundary sets S4 and G4 delimit  \nthe version space of all hypotheses consistent with the set of incrementally  \nobserved training  example s. \n \n \n \nInductive  bias:  \nDepartment  of CSE MRCET   \n \nDecision  Tre:e Decision  Trees  are a type of Supervised  Machine  Learning  (that \nis you explain what the input is and what the corresponding output is in the  \ntraining  data)  where  th e data is continuously split according to a certain  \nparameter. The tree can be explained by two entities, namely decision nodes and  \nleaves.  The leaves  are the decisions  or the final outcomes.  And the decision  nodes  \nare where  the data  is split.", "mimetype": "text/plain", "start_char_idx": 34708, "end_char_idx": 38645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ef7482e-55a8-4960-b0de-62afa976c5ec": {"__data__": {"id_": "2ef7482e-55a8-4960-b0de-62afa976c5ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53c03067-c46c-4d89-a622-e31d357a40db", "node_type": "1", "metadata": {}, "hash": "ed2d447a06bfd42f78011fcfa0cc5897210774929b4a575bc030931121d99fce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3658b62f-7129-4da9-8b34-07e460297fd2", "node_type": "1", "metadata": {}, "hash": "d2b3ffac57d5b5d99f6cbce2f89d28a55d78f0dd4c221d5c4af015dc02545c70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the decision  nodes  \nare where  the data  is split.  \n \nDecision  Tree  Representation:  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAn example of a decision tree can be explained using above binary tree. Let\u2019s say  \nyou want  to predict  whether  a person  is fit given  their information  like age, eating  \nhabit,  and physical activity,  etc. The decision  nodes here  are questions like  \n\u2018What\u2019s  the age?\u2019,  \u2018Does  he exercise?\u2019,  and \u2018Does  he eat a lot of pizzas\u2019?  And the \nleaves, which are outcomes like either \u2018fit\u2019, or \u2018unfit\u2019. In this case this was a  \nbinary  classification  problem  (a yes no type  problem).  There  are two main  types  \nof Decision  Trees:  \n \n1. Classification  trees  (Yes/No  types):  \n \nWhat  we have  seen above  is an example  of classification  tree, where  the \noutcome was a variable like \u2018fit\u2019 or \u2018unfit\u2019. Here the decision variable is  \nCategorical.  \nInductive bias refers to the restriction2s2 that are imposed by the assumptions \nDepartment  of CSE MRCET  \n23  \n  \nHere  the decision  or the outcome  variable  is Continuous,  e.g. a number  like \n123. Working Now that we know what a Decision Tree is, we\u2019ll see how it  \nworks  internally.  There  are many  algorithms  out there  which  construct  \nDecision Trees, but one  of the best is called as ID3 Algorithm. ID3  Stands  \nfor Iterative  Dichotomiser3.  \n \nBefore  discussing  the ID3 algorithm,  we\u2019ll  go through  few definitions.  \nEntropy, also called as Shannon Entropy is denoted by H(S) for a finite set S,  \nis the measure of  the amount  of uncertainty  or randomness in  data.  \n \nAppropriate  Problems  for Decision  Tree  Learning:  \n\u2022 Instances  are represented  by attribute -value  pair  \n\u2022 The target  function  has discrete  output  values  \n\u2022 Disjunctive  descriptions  may be required  \n\u2022 The training  data may contain  errors  \n\u2022 The training  data may contain  missing  attribute  values.  \n\u2022 Suitable  for classifications.  \n \nHypothesis  Space  Search:  \n \nThe set of possible  decision  tree, Simple  to complex,  hill climbing  search.  \nCapability:  \n\u2022 Hypothesis  space  of all decision  trees  is a complete  space  of finite  discrete  \nvalued functions.  \n \n\u2022 ID3 maintains  only a single  current  hypothesis.  \n \n\u2022 Cannot  determine  how many  alternative  decision  trees  are consistent  with \nthe available  training  data.  Department  of CSE MRCET  \n24  \n  \n\u2022 ID3 uses all training  example  at each step to make  statistically  based  \ndecisions regarding how  to refine  its current  hypothesis.  \n \n\u2022 The resulting  search  is much  less sensitive  to errors  in individual  training  \nexamples.", "mimetype": "text/plain", "start_char_idx": 38589, "end_char_idx": 41225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3658b62f-7129-4da9-8b34-07e460297fd2": {"__data__": {"id_": "3658b62f-7129-4da9-8b34-07e460297fd2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ef7482e-55a8-4960-b0de-62afa976c5ec", "node_type": "1", "metadata": {}, "hash": "80027a69b7aafb7c2605e83475d75133849e2f3ed3e9aa0daab13f3846077085", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74cc0a89-667d-40b0-bb62-9e0f0a34af85", "node_type": "1", "metadata": {}, "hash": "6248df41fbec9c1cadc4071b6bf42608864744cc825e719314a660f8a67db183", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Inductive Bias in Decision Tree Learning: Note H is the power set of  \ninstances  X \n \n\u2022 Inductive  Bias in ID3 \u2013 Approximate  inductive  bias of ID3  \n \n\uf0a2 Shorter  trees  are preferred  over larger  tress \n \n\uf0a2 BFS-ID3 \n \nDifference between (ID3 &  C-E) && Restriction bias and Preference  \nbias \nID3 Candidate -Elimination  \nSearches  a complete  hypothesis  space  \nincompletely  Searches  an incomplete  hypothesis  \nspace  completely  \nInductive  bias is solely  a consequence  \nof the ordering of hypotheses by its  \nsearch strategy  Inductive bias is solely a  \nconsequence of the expressive  \npower of its hypothesis  \nrepresentation  \nsss \nRestriction  bias Preference  bias \nCandidate -Elimination  ID3 \nCategorical  restriction  on the set of \nhypotheses considered  Preference for certain hypotheses  \nover others  Department  of CSE MRCET  \n25  \n  \nPossibility  of excluding  the unknown  \ntarget function  Work  within  a complete  hypothesis  \nspace  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nIssues  in Decision  Tree  Learning:  \n \n\u2022 Determine  how deeply  to grow  the decision  tree \n\u2022 Handling  continuous  attributes  \n\u2022 Choosing  an appropriate  attribute  selection  measure  \n\u2022 Handling  training  data with missing  attribute  values  \n\u2022 Handling  attributes  with differing  costs  \n\u2022 Improving  computational  efficiency  Department  of CSE MRCET  \n26  \n  \nUNIT -II \nArtificial Neural Networks  \nIntroduction:  \nArtificial Neural Networks (ANN) are algorithms based on brain function  \nand are used to model complicated patterns and forecast issues. The Artificial  \nNeural  Network  (ANN)  is a deep  learning  method  that arose  from  the \nconcept of the human brain Biological Neural Networks. The development of  \nANN was the result of an attempt to replicate the workings of the human  \nbrain. The workings of ANN are extremely si milar to those of biological  \nneural  networks,  although  they are not identical. ANN  algorithm  accepts  \nonly numeric and  structured  data.  \nThe ANN  applications:  \nClassification,  the aim is to predict  the class  of an input  vector  \n\u2022 Pattern  matching,  the aim is to produce  a pattern  best associated  with a given  \ninput  vector.  \n\u2022 Pattern  completion,  the aim is to complete  the missing  parts  of a given  input  \nvector.  \n\u2022 Optimization,  the aim is to find the optimal  values  of parameters  in an \noptimization  problem.  \n\u2022 Control,  an appropriate  action  is suggested  based  on given  an input  vectors  \n\u2022 Function  approximation/times  series  modelling,  the aim is to learn  the \nfunctional  relationships  between  input  and desired  output vectors.  \n\u2022 Data  mining,  with the aim of discovering  hidden  patterns  from  data \n(knowledge  discovery).  ANN  architectures  \n\u2022 Neural  Networks  are known  to be universal  function  approximators  \n\u2022 Various  architectures  are available  to approximate  any nonlinear  function  \n\u2022 Different  architectures  allow  for generation  of functions  of different  \ncomplexity and  power  \n\uf0a2 Feed  forward  networks  \n\uf0a2 Feedback  networks  \n\uf0a2 Lateral  networks  Department  of CSE MRCET  \n27  \n  \n \n \n \n \n \n \n \n \nAdvantages  of Artificial  Neural  Networks  \nAttribute -value  pairs  are used to represent  problems  in ANN.  \n1. The output  of ANNs  can be discrete -valued,  real-valued,  or a vector  of \nmultiple real or discrete -valued characteristics, while the target function can  \nbe discrete -valued, real -valued, or a vector of numerous real or discrete - \nvalued attributes.  \n2. Noise in t he training data is not a problem for ANN learning techniques.  \nThere may be mistakes in the training samples, but they will not affect the  \nfinal result.  \n3. It\u2019s utilized  when  a quick  assessment  of the taught  target  function  is \nnecessary.  \n4. The number  of weights  in the network.  \n5. the number  of training  instances  evaluated,  and the settings  of different  \nlearning algorithm parameters can all contribute to extended training periods  \nfor ANNs.  \nDisadvantages  of Artificial  Neural  Networks  \n1.", "mimetype": "text/plain", "start_char_idx": 41236, "end_char_idx": 45321, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74cc0a89-667d-40b0-bb62-9e0f0a34af85": {"__data__": {"id_": "74cc0a89-667d-40b0-bb62-9e0f0a34af85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3658b62f-7129-4da9-8b34-07e460297fd2", "node_type": "1", "metadata": {}, "hash": "d2b3ffac57d5b5d99f6cbce2f89d28a55d78f0dd4c221d5c4af015dc02545c70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "def1c3ec-48fb-4786-81ac-23d724976d6a", "node_type": "1", "metadata": {}, "hash": "227f78726e8dba48dc73a1b5507c2ccfafac11c1a9e7b93c3458d3ecb9dd700a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Disadvantages  of Artificial  Neural  Networks  \n1. Hardware  Dependence:  \n\u2022 The construc tion of Artificial  Neural  Networks  necessitates  the use of \nparallel  processors.  \n\u2022 As a result,  the equipment\u2019s  realization  is contingent.  \n2. Understanding  the network\u2019s  operation:  \n\u2022 This is the most  serious  issue  with ANN.  \n\u2022 When  ANN  provides  a probing  answer,  it does not explain  why or how it \nwas chosen.  \n\u2022 As a result,  the network\u2019s  confidence  is eroded.  \n3. Assured  network  structure:  Department  of CSE MRCET  \n28  \n  \n\u2022 Any precise  rule does not determine  the structure  of artificial  neural  \nnetworks.  \n\u2022 Experience  and trial and error  are used to develop  a suitable  network  \nstructure.  \n4. Difficulty  in presenting  the issue  to the network:  \n\u2022 ANNs  are capable  of working  with numerical  data.  \n\u2022 Before  being  introduced  to ANN,  problems  must  be converted  into \nnumerical values.  \n\u2022 The display  method  that is chosen  will have  a direct  impact  on the network\u2019s  \nperformance.  \n\u2022 The user\u2019s  skill is a factor  here.  \n5. The network\u2019s  lifetime  is unknown:  \u2022 When  the network\u2019s  error  on the \nsample  is decreased  to a specific  amount,  the training  is complete.  \n\u2022 The value  does not produce  the best outcomes.  \nAppropriate  Problems  for Neural  Network Learning:  \n1. Instances  are represented  by many  attribute -value  pairs  (e.g.,  the pixels  of a \npicture.  ALVINN  [Mitchell,  p. 84]).  \n2. The target  function  output  may be discrete -valued,  real-valued,  or a vector  of \nseveral real - or discrete -valued  attributes.  \n3. The training  examples  may contain  errors.  \n4. Long  training  times  are acceptable.  \n5. Fast evaluation  of the learned  target  function  may be required.  \n6. The ability  for humans  to understand  the learned  target  function  is not \nimportant.  \nHistory  of Neural  Networks:  \n1. 1943:  McCulloch  and Pitts proposed  a model  of a neuron  Perceptron  (read  \n[Mitchell,  section  4.4])  \n2. 1960s:  Widrow  and Hoff  explored  Perceptron  networks  (which  they called  \n\u201cAdelines\u201d)  and the delta rule.  \n3. 1962:  Rosenblatt  proved  the convergence  of the perceptron  training  rule. Department  of CSE MRCET  \n29  \n  \n4. 1969:  Minsky  and Papert  showed  that the Perceptron  cannot  deal with \nnonlinearly -separable  data sets ---even those that represent simple function  \nsuch as  X-OR. \n5. 1970 -1985:  Very  little research  on Neural  Nets \n6. 1986: Invention of Backpropagation Rumelhart and McClelland, but also  \nParker and earlier on: Werbos which can learn from nonlinearly -separab le \ndata sets. \n7. Since  1985: A  lot of research  in Neural  Nets ! \n \n \n \n \n \n \n \nMultilayer  Neural  Network:  \n\u2022 A multiplayer perceptron is a feed forward neural network with one or more  \nhidden  layers  \n\u2022 The network consists of an input layer of source neurons, at least one hidden  \nlayer  of computational  neurons,  and an output  layer  of computational  \nneurons.  \n\u2022 The input signals are propagated in a forward direction on a layer -by-layer  \nbasis.  \n\u2022 Neurons  in the hidde n layer  cannot  be observed  through  input/output  \nbehaviour  of the network.  \n\u2022 There is no obvious way to know what the desired output of the hidden layer  \nshould be.", "mimetype": "text/plain", "start_char_idx": 45270, "end_char_idx": 48578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "def1c3ec-48fb-4786-81ac-23d724976d6a": {"__data__": {"id_": "def1c3ec-48fb-4786-81ac-23d724976d6a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74cc0a89-667d-40b0-bb62-9e0f0a34af85", "node_type": "1", "metadata": {}, "hash": "6248df41fbec9c1cadc4071b6bf42608864744cc825e719314a660f8a67db183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef4200eb-7a67-4f13-996f-212d08e9d221", "node_type": "1", "metadata": {}, "hash": "ba8478bdb0135d1fd546e92978d0b46947799a32d419a34a34f1733c2e5b6efd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Department  of CSE MRCET  \n30  \n  \n \nDepartment  of CSE MRCET   \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n31 \nDepartment  of CSE MRCET  \n32  \n  \nBack  propagation:  Overview  \n\u2022 Back propagation works by applying the gradient descent rule to a feed  \nforward network.  \n\u2022 The algorithm is composed of two parts that get repeated over and over until  \na pre-set maximal  number of  epochs , EP max. \n\u2022 Part I, the feed forward pass: the activation v alues  of the hidden and then  \noutput units  are computed.  \n\u2022 Part II, the back propagation pass: the weights of the network are updated - \nstarting  with the hidden  to output  weights  and followed  by the  input  to \nhidden weights --with respect to the sum of squares error and through a series  \nof weight  update  rules  called  the Delta  Rule. \nDefinition:  \nThe Back propagation algorithm in neural network computes the gradient of  \nthe loss function for a single weight by the chain rule. It efficiently computes  \none layer at a t ime, unlike a native direct computation. It computes the  \ngradient, but it does not define how the gradient is used. It generalizes the  \ncomputation in  the delta rule.  \nConsider the following Back propagation neural network example diagram to  \nunderstand:  Department  of CSE MRCET  \n33  \n  \n \n\u2022 Inputs  X, arrive  through  the preconnected path  \n\u2022 Input  is modelled  using  real weights  W. The weights  are usually  randomly  \nselected.  \n\u2022 Calculate the output  for every  neuron  from  the input  layer, to  the hidden  \nlayers,  to the output  layer.  \n\u2022 Calculate  the error  in the outputs  \nError B= Actual  Output  \u2013 Desired  Output  \n\u2022 Travel  back  from  the output  layer  to the hidden  layer  to adjust  the weights  \nsuch that the error  is decreased.  \n\u2022 Keep  repeating  the process  until the desired  output  is achieved  \n \n \nWhy  We Need Back  propagation?  \n\u2022 Most  prominent  advantages  of Back  propagation  are: \n\u2022 Back  propagation is  fast, simple  and easy to program  \n\u2022 It has  no parameters  to tune  apart from  the numbers  of input \n\u2022 It is a flexible  method  as it does not require  prior  knowledge  about  the \nnetwork  \n\u2022 It is a standard  method  that generally  works  well \n\u2022 It does not need  any special  mention  of the features  of the function  to be \nlearned.   \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n1. Inputs  X, arrive  through  the preconnected  path \nDepartment  of CSE MRCET  \n34  \n \n \nTypes  of Back  propagation  Networks  \nTwo Types  of Back  propagation  Networks  are: \n\u2022 Static  Back -propagation  \n\u2022 Recurrent  Back  propagation  Static  back -propagation : \nIt is one kind of back propagation network which produces a mapping of a  \nstatic input for static output. It is useful to solve static classification issues  \nlike optical  character  recognition.  \nRecurrent  Back  propagation:  \nRecurrent Back propagation in data mining is fed forward until a fixed value  \nis achieved.  After  that, the error  is computed and  propagated backward.  \n \nDisadvantages  of using  Back  propagation  \n\u2022 The actual  performance  of back  propagation  on a specific  problem  is \ndependent on  the input  data. \n\u2022 Back  propagation  algorithm  in data mining  can be quite  sensitive  to noisy  \ndata \n\u2022 You need  to use the matrix -based  approach  for back  propagation  instead  of \nmini-batch.  \n \n \nBack  propagation:  The Algorithm  \n\u2022 Initialize  the weights  to small  random  values;  create  a random  pool of all the \ntraining  patterns;  set EP, the number  of epochs  of training to  0. \n\u2022 2. Pick a training  pattern  from the remaining  pool of patterns  and propagate  \nit forward  through  the network.  \n\u2022 3. Compute  the deltas,  k for the output  layer.  \n\u2022 4. Compute  the deltas,  \nbackward.", "mimetype": "text/plain", "start_char_idx": 48580, "end_char_idx": 52415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef4200eb-7a67-4f13-996f-212d08e9d221": {"__data__": {"id_": "ef4200eb-7a67-4f13-996f-212d08e9d221", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "def1c3ec-48fb-4786-81ac-23d724976d6a", "node_type": "1", "metadata": {}, "hash": "227f78726e8dba48dc73a1b5507c2ccfafac11c1a9e7b93c3458d3ecb9dd700a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b5ffa73-eeaf-47b6-a29c-d572cdd7ea42", "node_type": "1", "metadata": {}, "hash": "4e589b6465b6a2172647c75a9055ed31841daeb925acb04f71b61d5f60c240b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 4. Compute  the deltas,  \nbackward.  for the hidden  layer  by propagating  the error  \n\u2022 Update  all the connections  such that \n\u2022 W Newji = wjiold + wji and w Newkj = wkjOld + wkj \nj Department  of CSE MRCET  \n35  \n \n \n\u2022 If any pattern remains in the pool, then  go back to Step 2. If all the training  \npatterns in the pool have been used, then set EP = EP+1, and if EP  EPMax,  \nthen create a random pool of patterns and go to Step 2. If EP = EPMax, then  \nstop. \n \nBack  propagation:  The Momentum:  \n\u2022 To this point,  Back propagation  has the  disadvantage  of being too  slow if  is \nsmall  and it can oscillate  too widely  if is large.  \n\u2022 To solve  this problem,  we can add a momentum  to give each connection  \nsome  inertia,  forcing  it to change  in the direction  of the downhill  \u201cforce\u201d.  \n\u2022 New  Delta  Rule:  \nwpq(t+1)  = -    E/ wpq +    wpq(t)  \n\u2022 Where p and q are any input and hidden, or, hidden and  output units; t is a  \ntime step or epoch; and is the momentum parameter which regulates the  \namount  of inertia  of the weights.  \nDepartment  Of CSE MRCET  \n36  \n  \n \n \nUNIT  - III \n \nIntroduction  to Bayesian  Learning  \n \nImagine a situation where your friend gives you a new coin and asks you the  \nfairness of the coin (or the probability of observing heads) without even  \nflipping the coin once. In fact, you are also aware that your friend has not  \nmade the coin biased. In general, you have seen that coins are fair, thus you  \nexpect the probability of observing heads is 0.50.5. In the absence of any  \nsuch observations, yo u assert the fairness of the coin only using your past  \nexperiences  or observations  with coins.  \nSuppose  that you are allowed  to flip the coin 1010  times  in order  to \ndetermine the fairness of the coin. Your observations from the experiment  \nwill fall  under  one of the following  cases:  \n \n\u2022 Case  1: observing 55  heads  and 55  tails.  \n \n\u2022 Case  2: observing  hh heads  and 10\u2212h10\u2212h  tails,  where  h\u226010\u2212hh\u226010\u2212h.  \n \nIf case 1 is observed,  you are now more  certain  that the coin is a fair coin,  \nand you will decide that the probability of observing heads is 0.50.5 with  \nmore  confidence.  If case  2 is observed  you can either:  \n \n1. Neglect  your prior  beliefs  since  now you have  new data,  decide  the \nprobability of observing heads is h/10h/10 by solely depending on recent  \nobservations.  \n2. Adjust your belief accordingly to the value of hh that you have just observed,  \nand decide  the probability  of observing  heads  using  your recent  observations.  \n \nThe first method suggests that we use the frequentist method, where we  \nomit our beliefs when making decisions.  However, the second method  \nseems  to be more  convenient  because  1010  coins  are insufficient  to \ndetermine  the fairness  of a coin.  Therefore,  we can make  better  decisions  \nby combining our recent observations and beliefs that we have gained  \nthrough our past experiences. It is this thinking model which uses our most  \nrecent  observations  together  with our beliefs  or inclination  for critical  \nthinking that  is known  as Bayesian  thinking.  Department  Of CSE MRCET  \n37  \n  \n \n \nMoreover, assume that your friend allows you to conduct another 1010 coin  \nflips. Then we can use these new observations to further update our beliefs.  \nAs we gain more data, we can incrementally update our beliefs  increasing  \nthe certainty of our conclusions . This is known as incremental learning,  \nwhere  you update  your knowledge  incrementally  with new evidence.  \nBayesian  learning  comes  into play on such occasions,  where  we are unable  \nto use frequentist statistics due to the drawbacks that we have discussed  \nabove.", "mimetype": "text/plain", "start_char_idx": 52378, "end_char_idx": 56101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b5ffa73-eeaf-47b6-a29c-d572cdd7ea42": {"__data__": {"id_": "8b5ffa73-eeaf-47b6-a29c-d572cdd7ea42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef4200eb-7a67-4f13-996f-212d08e9d221", "node_type": "1", "metadata": {}, "hash": "ba8478bdb0135d1fd546e92978d0b46947799a32d419a34a34f1733c2e5b6efd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "733503e8-6e1d-4351-8991-db2d630c56dd", "node_type": "1", "metadata": {}, "hash": "78b3f36f852a4cca2bf424ed63ccd90a0f3c03dff2038a8f0518a4ae18e5713a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can use Bayesian  learning  to address  all these  drawbacks  and \neven  with additional  capabilities  (such  as incremental  updates  of the \nposterior) when testing a hypothesis to estimate unknown parameters of a  \nmachine  learning  models.  Bayesian  learning  uses Bayes\u2019  theorem  to \ndetermine  the conditional  probability  of a hypotheses  given  some  evidence  \nor observations.  \nThe Famous  Coin Flip  Experiment  \n \nWhen we flip a coin, there are two possible outcomes - heads or tails. Of  \ncourse, there is a third rare poss ibility where the coin balances on its edge  \nwithout falling onto either side, which we assume is not a possible outcome  \nof the coin flip for our discussion. We conduct a series of coin flips and  \nrecord our observations i.e. the number of the heads (or tail s) observed for a  \ncertain number of coin flips. In this experiment, we are trying to determine  \nthe fairness  of the coin,  using  the number  of heads  (or tails)  that we observe.  \n \nFrequentist  Statistics  \n \nLet us think about how we can determine the fairness of the coin using our  \nobservations in the above mentioned experiment. Once we have conducted a  \nsufficient number of coin flip trials, we can determine the frequency or the  \nprobability of observing the heads (or tails). If we observed heads and tails  \nwith equa l frequencies or the probability of observing heads (or tails) is  \n0.50.5,  then it can be established  that the coin is a fair coin.  Failing  that, it is \na biased coin. Let's denote pp as the probability of observing the heads.  \nConsequently, as the quantity that pp deviates from 0.50.5 indicates how  \nbiased  the coin is, pp can be considered as  the degree -of-fairness  of the coin.  Department  Of CSE MRCET  \n38  \n  \n \n \nTesting  whether  a hypothesis  is true or false  by calculating  the probability  \nof an event in a prolonged experiment is known as frequentist statistics . As \nsuch,  determining  the fairness  of a coin by using  the probability  of \nobserving the heads is an example of frequentist statistics (a.k.a. frequentist  \napproach ). \nLet us now further investigate the coin flip example using the frequentist  \napproach . Since we have not intentionally altered the coin, it is reasonable to  \nassume that we are using an unbiased coin for the experiment. When we flip  \nthe coin 1010  times,  we observe  the heads  66 times.  Therefore,  the pp is \n0.60.6 (note that pp is the number of heads observed over the number of total  \ncoin flips).  Hence,  according  to frequencies  statistics,  the coin is a biased  \ncoin \u2014 which opposes our assumption of a fair coin. Perhaps one of your  \nfriends who is more skeptical than you extends this experiment to 100100  \ntrails  using  the same  coin.  Then  she observes  heads  5555 times,  which  \nresults  in a different  pp with 0.550.55.  Even  though  the new value  for pp \ndoes not change our previous conclusion  (i.e. that the coin is biased), this  \nobservation raises  several  questions:  \n \n\u2022 How  confident  are we of pp being  0.60.6?  \n \n\u2022 How  confident  are of pp being  0.550.55?  \n \n\u2022 Which  of these  values  is the accurate  estimation  of pp? 39  \n Department  of CSE MRCET  \nWill pp continue  to change  when  we further  increase  the number  of coin flip \ntrails?  \n \nWe cannot  find out the exact  answers  to the first three  questions  using  \nfrequentist  statistics .  We  may assume  that true value  of pp is closer  to \n0.550.55 than 0.60.6 because the former is computed using observations from  \na considerable number of trials compared to what we used to compute the  \nlatter. Yet there is no way of confirming that hypothesis. However, if we  \nfurther increase the number of trials, we may ge t a different probability from  \nboth of the above values  for observing  the heads  and eventually, we may  \neven discover  that the coin is  a fair coin.", "mimetype": "text/plain", "start_char_idx": 56103, "end_char_idx": 60014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "733503e8-6e1d-4351-8991-db2d630c56dd": {"__data__": {"id_": "733503e8-6e1d-4351-8991-db2d630c56dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b5ffa73-eeaf-47b6-a29c-d572cdd7ea42", "node_type": "1", "metadata": {}, "hash": "4e589b6465b6a2172647c75a9055ed31841daeb925acb04f71b61d5f60c240b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c431cfa-3853-44e9-bb21-1c1245c36752", "node_type": "1", "metadata": {}, "hash": "da7d5cdd85040d8d70a26c0fb06b3ed699dce639f48f1164eaf31ea4bccf50ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Number  of coin Number  of heads  Probability  of observing  heads   \nflips     \n10   6 0.6 \n50   29 0.58 \n100   55 0.55 \n200   94 0.47 \n500   245 0.49 \nTable  1 - Coin  flip experiment  results  when  increasing  the number  of \ntrials  \n \n \nTable 1 presents some of the possible outcomes of a hypothetical coin flip  \nexperiment when we are increasing the number of trials. The  fairness (pp) of  \nthe coin changes when increasing the number of coin -flips in this experiment.  \nOur confidence  of estimated  pp may also increase  when  increasing  the \nnumber  of coin-flips,  yet the frequentist  statistic  does not facilitate  any \nindication of t he confidence of the estimated pp value. We can attempt to  \nunderstand  the importance  of such a confident  measure  by studying  the \nfollowing cases:  \n \n\u2022 An experiment with an infinite number of trials guarantees pp with absolute  \naccuracy (100% confidence). Yet, it is not practical to conduct an experiment  \nwith an infinite number of trials and we should stop the experiment after a  \nsufficiently  large  number  of trials.  However,  deciding  the value  of this \nsufficient  number  of trials  is a challenge  when  using  frequent ist statistics . \nIf we can determine the confidence of the estimated pp value or the inferred  \nconclusion,  in a situation where the  number  of trials  is limited,  this will allow  Department  Of CSE MRCET  \n40  \n  \nus to  decide whether to  accept the conclusion or to  extend the experiment  \nwith more  trials  until it achieves  sufficient  confidence.  \n \nMoreover, we may have valuable insights or prior beliefs (for example, coins  \nare usually fair and the coin used is not made biased intentionally, therefore  \np\u22480.5p\u22480.5) that describes the value of pp. Embedding that information can  \nsignificantly improve the accuracy of the final conclusion. Such beliefs play a  \nsignificant role in shaping the outcome of a hypothesis test especially when  \nwe have limited data. Ho wever, with frequentist statistics , it is not possible to  \nincorporate such beliefs or past experience to increase the accuracy of the  \nhypothesis test.  \nSome  Terms  to Understand  \n \nBefore  delving  into Bayesian  learning,  it is essential  to understand  the \ndefinition of some terminologies used. I will not provide lengthy explanations  \nof the mathematical definition since there is a lot of widely available content  \nthat you can use to understand  these  concepts.  \n \n\u2022 Random variabl e (Stochasti c variable)  - In statistics, the random variable is a  \nvariable whose possible values  are a result of a random event. Therefore,  \neach possible value of a random variable has some  probability attached to it  \nto represent  the likelihood  of those  values.  \n\u2022 Probability distribution  - The function that defines the probability of different  \noutcomes/values  of a random  variable.  The continuous  probability  \ndistributions  are described  using  probability  density  functions  whereas  \ndiscrete probability distributions can be represented using probability mass  \nfunctions.  \nConditiona l probability  - This is a measure of probability P(A|B)P(A|B) of an  \nevent  A given that  another event B  has occurred.  \n\u2022 Joint probability  distribution  \n \n \nBayes\u2019  Theorem  \n \nBayes\u2019 theorem describes how the conditional probability of an event or a  \nhypothesis can be computed using evidence and prior knowledge. It is similar  \nto concluding  that our code  has no bugs  given  the evidence  that it has passed  Department  Of CSE MRCET  \n41  \n  \nall the test cases, including our prior belief that we have rarely observed any  \nbugs in our code. However, this intuition goes beyond that simple hypothesis  \ntest where there are multiple events or hypotheses involved (let us not worry  \nabout this  for the moment).  \n \nThe Bayes\u2019  theorem  is given  by: \n \n \nP(\u03b8|X)=P(X|\u03b8)P(\u03b8)P(X)P(\u03b8|X)=P(X|\u03b8)P(\u03b8)P(X)  \n \n \nI will now explain each term in Bayes\u2019 theorem using the above example.  \nConsider the hypothesis that there are no bugs in our code. \u03b8\u03b8 and XX denote  \nthat our code  is bug free  and passes all the  test cases respectively.", "mimetype": "text/plain", "start_char_idx": 60017, "end_char_idx": 64172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c431cfa-3853-44e9-bb21-1c1245c36752": {"__data__": {"id_": "4c431cfa-3853-44e9-bb21-1c1245c36752", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "733503e8-6e1d-4351-8991-db2d630c56dd", "node_type": "1", "metadata": {}, "hash": "78b3f36f852a4cca2bf424ed63ccd90a0f3c03dff2038a8f0518a4ae18e5713a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6397dd93-c879-49f4-bf31-8d8576c3c3db", "node_type": "1", "metadata": {}, "hash": "7bb60e343b5689d9595c058839af19d5cb681e61b8cb310807ed1ba5f6723180", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 P(\u03b8)P(\u03b8) - Prior Probability is the probability of the hypothesis \u03b8\u03b8 being true  \nbefore applying the Bayes\u2019 theorem. Prior represents the beliefs that we have  \ngained through past experience, which refers to either common sense or an  \noutcome of Bayes\u2019 theorem for some past observations. For the example  \ngiven, prior probability denotes the probability of observing  no bugs in our  \ncode. However, since  this is the  first time  we are applying Bayes\u2019 theorem,  \nwe have to  decide  the priors  using  other  means  \n(Otherwise we could use the previous posterior as the new prior). Let us  \nassume that it  is very unlikely to find bugs  in our code because rarely have  \nwe observed  bugs  in our code  in the past.  With  our past experience  of \nobserving fewer bugs in our code, we can assign our prior P(\u03b8)P(\u03b8) with a  \nhigher  probability.  However,  for now,  let us assume  that P(\u03b8)=pP(\u03b8)  \nThis term depends on the test coverage of the test cases. Even though we do  \nnot know the value of this term without proper measurements, in order to  \ncontinue  this discussion  let us assume  that P(X|\u00ac\u03b8)=0.5P(X|\u00ac\u03b8)=0.5.  \nAccordingly,  \nP(X)=1\u00d7p+0.5\u00d7(1\u2212p)=0.5(1+p)P(X)=1\u00d7p+ 0.5\u00d7(1\u2212p)=0.5(1+p)  \n \n\u2022 P(\u03b8|X)P(\u03b8|X)  - Posteriori probability denotes the  conditional probability of  \nthe hypothesis \u03b8\u03b8 after observing the evidence XX. This is the probability of  \nobserving  no bugs  in our code  given  that it passes  all the test cases.  Since  we Department  Of CSE MRCET  \n42  \n  \nnow know the values for the other three terms in the Bayes\u2019 theorem, we can  \ncalculate  the posterior  probability using the  following formula:  \n \nP(\u03b8|X)=1\u00d7p0.5(1+p)P(\u03b8|X)=1\u00d7p0.5(1+p)  \nWe can also calculate  the probability  of observing  a bug, given  that our code  \npasses all  the test cases  P(\u00ac\u03b8|X)P(\u00ac\u03b8|X) .  \nP(\u00ac\u03b8|X)=P(X|\u00ac\u03b8).P(\u00ac\u03b8)P(X)=0.5\u00d7(1\u2212p)0.5\u00d7(1+p)=(1\u2212p)(1+p)P(\u00ac\u03b8|X)=P(X|\u00ac  \n\u03b8).P(\u00ac\u03b8)  \nP(X)=0.5\u00d7(1\u2212p)0.5\u00d7(1+p)=(1\u2212p)(1+p)  \nWe now know both conditional probabilities of observing a bug in the code  \nand not observing the bug in the code. Yet how are we going to confirm the  \nvalid hypothesis  using these posterior  probabilities?  \n \nMaximum  a Posteriori  (MAP)  \n \nWe can use MAP to determine the valid hypothesis from a set of hypotheses.  \nAccording  to MAP,  the hypothesis  that has the maximum  posterior  \nprobability is considered as the valid hypothesis. Therefore, we can express  \nthe hypothesis  \u03b8MAP\u03b8MAP  that is concluded  using  MAP  as follows:  \n\u03b8MAP=argmax\u03b8P(\u03b8i|X)=argmax\u03b8(P(X|\u03b8i)P(\u03b8i)P(X))\u03b8MAP=argmax\u03b8P(\u03b8i|X)  \n=argmax\u03b8(P(X|\u03b8  i)P(\u03b8i)P(X))  \nThe argmax\u03b8argmax\u03b8  operator  estimates  the event  or hypothesis  \u03b8i\u03b8i that \nmaximizes  the posterior  probability  P(\u03b8i|X)P(\u03b8i|X).", "mimetype": "text/plain", "start_char_idx": 64177, "end_char_idx": 66886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6397dd93-c879-49f4-bf31-8d8576c3c3db": {"__data__": {"id_": "6397dd93-c879-49f4-bf31-8d8576c3c3db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c431cfa-3853-44e9-bb21-1c1245c36752", "node_type": "1", "metadata": {}, "hash": "da7d5cdd85040d8d70a26c0fb06b3ed699dce639f48f1164eaf31ea4bccf50ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b60359c-a455-4130-8861-a5fd25955fbd", "node_type": "1", "metadata": {}, "hash": "bb7cb05ed39c3d1f51e07f927351816c135e888fa2b45df993a89f2057470de5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let us apply  MAP  to the \nabove example in order to determine the true hypothesis:  \n\u03b8MAP=argmax\u03b8{\u03b8:P(\u03b8|X)=p0.5(1+p),\u00ac\u03b8:P(\u00ac\u03b8|X)=(1\u2212p)(1+p)}\u03b8MAP=argma  \nx\u03b8{\u03b8:P(\u03b8|X)=p0.5(1+p),\u00ac\u03b8:P(\u00ac\u03b8|X)=(1\u2212p)(1+p)}  Department  Of CSE MRCET  \n43  \n  \n \n \n \n \n \n \n \n \n \n \n \nFigure  1 - P(\u03b8|X)P(\u03b8|X)  and P(\u00ac\u03b8|X)P(\u00ac\u03b8|X)  when  changing  the \nP(\u03b8)=pP(\u03b8)=p Figure 1 illustrates how the posterior probabilities of possible  \nhypotheses change with the value of prior probability. Unlike  frequentist  \nstatistics  where  our belief  or past experience  had no influence  on the \nconcluded  hypothesis,  Bayesian  learning  is capable  of incorporating  our \nbelief to improve the accuracy of predictions. Assuming that we have fairly  \ngood  programmers  and therefore  the probability  of observing  a bug is \nP(\u03b8)=0.4P(\u03b8)=0.4 ,  then we find the \u03b8MAP\u03b8MAP:  \nMAP=argmax\u03b8{\u03b8:P(|X)=0.40.5(1+0.4),\u00ac\u03b8:P(\u00ac\u03b8|X)=0.5(1\u22120.4)0.5(1+0.4)}=ar  \ngmax\u03b8 {\u03b8:P(\u03b8|X)=0.57,\u00ac\u03b8:P(\u00ac\u03b8|X)=0.43}=\u03b8 \u27f9No bugs  present  in our \ncodeMAP=argmax\u03b8{\u03b8:P(|X)=0.40.5(1+0.4),\u00ac\u03b8:P(\u00ac\u03b8|X)=0.5(1\u22120.4)0.5(1+0.4  \n)}=argmax\u03b8{\u03b8:P(\u03b8|X)=0.57,\u00ac\u03b8:P(\u00ac\u03b8|X)=0.43}=\u03b8 \u27f9No bugs  present  in our \ncode  \nDepartment  Of CSE MRCET  \n44  \n  \nHowever, P(X)P(X) is independent of \u03b8\u03b8, and thus P(X)P(X) is same for all  \nthe events  or hypotheses.  Therefore,  we can simplify  the \u03b8MAP\u03b8MAP  \nestimation, without the denominator of each posterior computation as shown  \nbelow:  \u03b8MAP=argmax\u03b8(P(X|\u03b8i)P(\u03b8i))\u03b8MAP=argmax\u03b8(P(X|\u03b8i)P(\u03b8i))  \nNotice that MAP estimation algorithms do not compute posterior probability  \nof each hypothesis  to decide  which  is the most  probable  hypothesis.  \nAssuming that our hypothesis space is continuous (i.e. fairn ess of the coin  \nencoded as probability of observing heads, coefficient of a regression model,  \netc.),  where  endless  possible  hypotheses  are present  even in  the smallest  \nrange that the human mind can think of, or for even a discrete hypothesis  \nspace with a l arge number of possible outcomes for an event, we do not need  \nto find the posterior of each hypothesis in order to decide which is the most  \nprobable  hypothesis.  Therefore,  the practical  implementation  of MAP  \nestimation algorithms use approximation techniques, which are capable of  \nfinding  the most  probable  hypothesis  without  computing  posteriors  or only \nby computing  some of them.  \n \nUsing the Bayesian theorem, we can now incorporate our belief as the prior  \nprobability,  which  was not possible  when  we used frequentist  statistics . \nHowever, we still have the problem of deciding a sufficiently large number of  \ntrials or attaching a confidence to the concluded hypothesis. This is because  \nthe above  example  was solely  designed  to introduce  the Bayesian  theorem  \nand each of  its terms.  Let us now gain a better understanding  of \nBayesian  learning  to learn  about  the full potential  of Bayes\u2019  theorem.  \n \nBinomial  Likelihood  \nThe likelihood for the coin flip experiment is given by the probability of  \nobserving heads out of a ll the coin flips given the fairness of the coin.", "mimetype": "text/plain", "start_char_idx": 66888, "end_char_idx": 69951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b60359c-a455-4130-8861-a5fd25955fbd": {"__data__": {"id_": "4b60359c-a455-4130-8861-a5fd25955fbd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6397dd93-c879-49f4-bf31-8d8576c3c3db", "node_type": "1", "metadata": {}, "hash": "7bb60e343b5689d9595c058839af19d5cb681e61b8cb310807ed1ba5f6723180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "407a718c-48b7-459d-a287-028f6f1899a8", "node_type": "1", "metadata": {}, "hash": "8dfee9e32b8d1b508466a0d188a0c88dee4f2acdd546546445502ad27d9c10a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we  \nhave defined the fairness of the coins (\u03b8\u03b8) using the probability of observing  \nheads  for each coin flip, we can define  the probability  of observing  heads  or Department  Of CSE MRCET  \n45  \n  \ntails given  the fairness  of the coin P(y|\u03b8)P(y|\u03b8)  where  y=1y=1  for observing  \nheads and y=0y=0 for observing tails. Accordingly:  \nP(y=1|\u03b8)=\u03b8P(y=0|\u03b8)=(1\u2212\u03b8)P(y=1|\u03b8)=\u03b8P(y=0|\u03b8)=(1\u2212\u03b8)  \nNow  that we have  defined  two conditional  probabilities  for each outcome  \n \nabove,  let us now try to find the P(Y=y|\u03b8)P(Y=y|\u03b8)  joint probability  of \nobserving heads  or tails:  \nP(Y=y|\u03b8)={\u03b8,  if y=11\u2212\u03b8,  otherwise  P(Y=y|\u03b8)={\u03b8,  if y=11\u2212\u03b8,  otherwise  \nNote that yy can only take either 00 or 11, and \u03b8\u03b8 will lie within the range of  \n[0,1][0,1]. We can rewrite the a bove expression in a single expression as  \nfollows:  \nP(Y=y|\u03b8)=\u03b8y\u00d7(1\u2212\u03b8)1\u2212yP(Y=y|\u03b8)=\u03b8y\u00d7(1\u2212\u03b8)1\u2212y  \nThe above  equation  represents  the likelihood  of a single  test coin flip \nexperiment.  \nInterestingly, the likelihood function of the single coin flip experiment is  \nsimilar to the Bernoulli probability distribution. The Bernoulli distribution is  \nthe probability  distribution  of a single  trial experiment  with only two \nopposite   outcomes.   As   the   Bernoull i   probability    distributio n   is   the \nsimplification  of Binomia l probabilit y distribution  for a single  trail, we can \n \nrepresent the likelihood of a coin flip experiment that we ob serve kk number  \nof heads out of NN number of trials as a Binomial probability distribution as  \nshown  below:  \nP(k,N|\u03b8)=(Nk)\u03b8k(1\u2212\u03b8)N\u2212k  Department  Of CSE MRCET  \n46  \n  \nMaximum  likelihood  estimation  method  (MLE)  \n \nThe likelihood function indicates how likely the observed sample is as a  \nfunction of possible parameter values. Therefore, maximizing the likelihood  \nfunction  determines  the parameters  that are most  likely  to produce  the \nobserved data. From a statistical point of view, MLE is usually recommended  \nfor large  samples  because  it is versatile,  applicable  to most  models  and \ndifferent  types of  data,  and produces  the most precise  estimates.  \n \nLeast  squares  estimation  method  (LSE)  \n \nLeast squares estimates are calculated by fitting a regression line to the points  \nfrom  a data set that has the minimal sum of the deviations squared (least  \nsquare error). In reliability analysis, the line and the data are plotted on a  \nprobability plot.  \nBayes  Optimal  Classifier  \n \nThe Bayes optimal classifier is a probabilistic model that make s the most  \nprobable  prediction for  a new example,  given the  training  dataset.  \n \nThis model  is also referred  to as the Bayes  optimal  learner,  the Bayes  \nclassifier,  Bayes  optimal  decision  boundary,  or the Bayes  optimal  \ndiscriminant function.  \n \nGibbs  Sampling  Algorithm  \nWe start off by selecting an initial value for the random variables X & Y. \nThen, we sample from the conditional probability distribution of X given Y =  \nY\u2070 denoted p(X|Y\u2070). In the next step, we sample a new value of Y conditional  \non X\u00b9, which we j ust computed. We repeat the procedure for an additional n - \n1 iterations, alternating between drawing a new sample from the conditional  \nprobability  distribution of  X and the conditional  probability  distribution  of Y,  \ngiven  the current  value  of the other  random  variable.  Department  Of CSE MRCET  \n47  \n  \nLet\u2019s  take a look at an example.  Suppose  we had the following  posterior  and \nconditional  probability  distributions.", "mimetype": "text/plain", "start_char_idx": 69952, "end_char_idx": 73475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "407a718c-48b7-459d-a287-028f6f1899a8": {"__data__": {"id_": "407a718c-48b7-459d-a287-028f6f1899a8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b60359c-a455-4130-8861-a5fd25955fbd", "node_type": "1", "metadata": {}, "hash": "bb7cb05ed39c3d1f51e07f927351816c135e888fa2b45df993a89f2057470de5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "949714ca-56c3-48d7-ba3a-5337c63a519a", "node_type": "1", "metadata": {}, "hash": "da2df8e71b5e106190f9cf75dace1c071a264777cae7fa91d8af83909ef7d057", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Naive  Bayes  Classifier  Algorithm  \n\u2022 Na\u00efve  Bayes  algorithm  is a supervised  learning  algorithm,  which  is based  on \nBayes  theorem  and used for solving  classification  problems.  \nDepartment  Of CSE MRCET  \n48  \n  \n\u2022 It is mainly  used in text classification  that includes  a high-dimensional  \ntraining  dataset.  \n\u2022 Na\u00efve Bayes Classifier is one of the simple and most effective Classification  \nalgorithms which helps in building the fast machine learning models that can  \nmake  quick  predictions.  \n\u2022 It is a probabilistic classifier, which means it predicts on the basis of the  \nprobability  of an object . \n\u2022 Some popular examples  of Na\u00efve Bayes Algorithm are spam filtration,  \nSentimental analysis,  and classifying  articles . \nEXAMPLE  \nSuppose we have a dataset of weather conditions and corresponding target  \nvariable \" Play \". So using this dataset we need to decide that whether we  \nshould play or not on a particular day according to the weather conditions. So  \nto solve this  problem,  we need  to follow  the below steps:  \n \n1. Convert  the given  dataset  into frequency  tables.  \n2. Generate  Likelihood  table  by finding  the probabilities  of given  features.  \n3. Now, use Bayes theorem to calculate the posterior probability.  \nProblem : If the weather is sunny, then the Player should play or not?  \nSolution : To solve  this, first consider  the below  dataset:  \n \nOutlook    \nPlay  \n0 Rainy  Yes \n1 Sunny  Yes \n \n2  \nOvercast   \nYes \n3 Overcast  Yes \n \n4  \nSunny   \nNo \n5 Rainy  Yes \n \n6  \nSunny   \nYes \n  \nDepartment Of  CSE MRCET  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFrequency  table  for the Weather  Conditions:  \n \n \n \n \n \n \n \n \n \n \n \nLikelihood  table  weather  condition:  \nWeather  No Yes   \n \nOvercast   \n0  \n5  \n5/14= 0.35 \nRainy  2 2 4/14=0.29  \n \nSunny   \n2  \n3  \n5/14=0.35  \nAll 4/14=0.29  10/14=0.71   \n \nApplying  Bayes'theorem:  \n \nP(Yes|Sunny)=  P(Sunny|Yes)*P(Yes)/P(Sunny)  \n49 \n 7 Overcast  Yes \n \n8  \nRainy   \nNo \n9 Sunny  No \n \n10  \nSunny   \nYes \n11 Rainy  No \n \n12  \nOvercast   \nYes \n13 Overcast  Yes \n \n \nWeather   \nYes  \nNo \n \nOvercast   \n5  \n0 \n \nRainy   \n2  \n2 \n \nSunny   \n3  \n2 \nTotal  10 5 \n Department  Of CSE MRCET  \n50  \n  \nP(Sunny|Yes)=  3/10=  0.3 \n \nP(Sunny)= 0.35  \nP(Yes)=0.71  \nSo P(Yes|Sunny) = 0.3*0.71/0.35= 0.60 \nP(No|Sunny)= P(Sunny|No)*P(No)/P(Sunny)  \nP(Sunny|NO)=  2/4=0.5  \nP(No)=  0.29 \n \nP(Sunny)=  0.35 \n \nSo P(No|Sunny)=  0.5*0.29/0.35  = 0.41 \n \nBayesian  Belief  Network:  \n \nIt is a graphical  representation  of different  probabilistic  relationships  among  \nrandom  variables  in a  particular  set. It is a classifier  with no dependency  on \nattributes i.e it is condition independent. Due to its feature of joint probability, the  \nprobability  in Bayesian  Belief  Network  is derived,  based  on a condition  \u2014 \nP(attribute/parent)  i.e probability of  an attribute, true  over parent  attribute.  \n \nConsider  this example:  \n \n \n \n \n\u2022 In the above  figure,  we have an alarm  \u2018A\u2019 \u2013 a node,  say installed  in a house  \nof a person  \u2018gfg\u2019, which  rings  upon  two probabilities  i.e burglary  \u2018B\u2019 and fire \nDepartment  Of CSE MRCET  \n51  \n  \n\u2018F\u2019, which are \u2013 parent nodes of the alarm node.", "mimetype": "text/plain", "start_char_idx": 73504, "end_char_idx": 76710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "949714ca-56c3-48d7-ba3a-5337c63a519a": {"__data__": {"id_": "949714ca-56c3-48d7-ba3a-5337c63a519a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "407a718c-48b7-459d-a287-028f6f1899a8", "node_type": "1", "metadata": {}, "hash": "8dfee9e32b8d1b508466a0d188a0c88dee4f2acdd546546445502ad27d9c10a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52e45947-6718-421f-bded-d83055e36d97", "node_type": "1", "metadata": {}, "hash": "fea2df02a4df0968413a1d4eda1bfb6e7a48ce0bb8d649e805e84d6ee1d8781f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The alarm is the parent node  \nof two probabilities P1 calls  \u2018P1\u2019 & P2 calls  \u2018P2\u2019 person nodes.  \n \n\u2022 Upon the instance of  burglary and fire, \u2018P1\u2019 and \u2018P2\u2019 call person \u2018gfg\u2019,  \nrespectively. But, there are few drawbacks in this case, as sometimes \u2018P1\u2019  \nmay forget to call the person \u2018gfg\u2019, even after hearing the alarm, as he has a  \ntendency to forget things, quick.  Similarly, \u2018P2\u2019, sometimes fails to call the  \nperson \u2018gfg\u2019,  as he  is only able  to hear the alarm,  from  a certain  distance.  \nExpectation -Maximization  Algorithm  \nIn the real -world applications of machine learning, it is very common that  \nthere are many relevant features available for learning but only a small subset  \nof them are observable. So, for the variables which are sometimes observable  \nand sometimes  not, then  we  can use the instances  when  that variable  is \nvisible is observed for the purpose of learning and then predict its value in the  \ninstances  when  it is not observable.  \nOn the other hand, Expectation -Maximization algorithm can be used for the  \nlatent variables (variables that are not directly observable and are actually  \ninferred from the values of the other observed variables) too in order to  \npredict their values with the condition that the general form of probability  \ndistribution governing those latent variables is known to us. This algorithm is  \nactually at the base of many unsupervised clustering algorithms in the field of  \nmachine  learning.  \nIt was explained,  proposed  and given  its name  in a paper  published  in 1977  \nby Arthur Dempster, Nan Laird, and Donald Rubin. It is used to find the local  \nmaximum likelihood parameters of a statistical model in the cases where  \nlatent variables are  involved and the  data is missing or  incom plete.  \n \n \nAlgorithm:  \n1. Given  a set of incomplete  data,  consider  a set of starting parameters.  \n2. Expectation  step (E \u2013 step):  Using  the observed  available  data of the \ndataset,  estimate (guess)  the values  of the missing data.  \n3. Maximization  step (M \u2013 step):  Complete  data generated  after the \nexpectation (E)  step is  used in  order  to update  the parameters.  \n4. Repeat  step 2 and step 3 until convergence.  Department  Of CSE MRCET  \n52  \n  \n \nThe essence of Expectation -Maximization algorithm is to use the available  \nobserved data  of the dataset to estimate the missing data and then using that  \ndata to update  the values  of the parameters.  Let us understand  the EM \nalgorithm  in detail.  \n\u2022 Initially, a set of initial values of the parameters are considered. A set of  \nincomplete observed data is given to the system with the assumption that the  \nobserved data comes  from a  specific model.  \n\u2022 The next step is known as \u201cExpectation\u201d \u2013 step or E-step. In this step, we use  \nthe observed data in order to estimate or guess the values of the missing or  \nincomplete  data.  It is basically  used to update  the variables.  \n\u2022 The next step is known  as \u201cMaximization\u201d -step or M-step. In this step,  we \nuse the complete data generated  in the preceding \u201cExpectation\u201d  \u2013 step in  \norder to update the values of the parameters. It is basically used to update the  \nhypothesis.  \n\u2022 Now, in the fourth step, it is checked whether the values are converging or  \nnot, if yes, then stop otherwise repeat step-2 and step-3 i.e. \u201cExpectation\u201d \u2013 \nstep and  \n\u201cMaximization\u201d  \u2013 step until the convergence  occurs.  \n \nFlow  chart  for EM algorithm  \nDepartment  Of CSE MRCET  \n53  \n  \n \n \n \n \n \n \n \n \n \n \n \nUsage  of EM algorithm  \n\u2022 It can be  used to fill  the missing data  in a sample.  \n\u2022 It can be used as  the basis  of unsupervised  learning  of clusters.  \n\u2022 It can be used for the purpose of estimating the parameters of Hidden Markov  \nModel (HMM).  \n\u2022 It can be used for discovering  the values  of latent  variables.  \n \nAdvantages  of EM algorithm  \n\u2022 It is always  guaranteed  that likelihood  will increase  with each iteration.", "mimetype": "text/plain", "start_char_idx": 76711, "end_char_idx": 80679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52e45947-6718-421f-bded-d83055e36d97": {"__data__": {"id_": "52e45947-6718-421f-bded-d83055e36d97", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "949714ca-56c3-48d7-ba3a-5337c63a519a", "node_type": "1", "metadata": {}, "hash": "da2df8e71b5e106190f9cf75dace1c071a264777cae7fa91d8af83909ef7d057", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b93c396-2f42-4bb5-bff3-81fdbbe6546c", "node_type": "1", "metadata": {}, "hash": "8a283940f791ec78d7be38d01cbeb8ea0b2e9b26e3185c1255ea2484822160d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The E -step and M -step are often pretty easy for many problems in terms of  \nimplementation.  \n\u2022 Solutions  to the M-steps  often exist in  the closed form.  \nDepartment  Of CSE MRCET  \n54  \n  \nInstance -based  learning  \nThe Machin e Learning  systems which are categorized as instance -based  \nlearning are the systems that learn the training examples by heart and then  \ngeneralizes to new instances based on some similarity measure. It is called  \ninstance -based because it builds the hypotheses from the training instances.  \nIt is also known as memory -based learning or lazy-learning. The time  \ncomplex ity of this algorithm depends upon the size of training data. The  \nworst -case time complexity  of this algorithm  is O (n), where  n is the \nnumber  of training  instances.  \nFor example,  If we were  to create  a spam  filter  with an instance -based  \nlearning algorithm,  instead of just flagging emails that are already marked as  \nspam emails, our spam filter would be programmed to also flag  emails that  \nare very  similar to  them. This  requires  a measure of resemblance between  \ntwo emails. A similarity measure between two emails could be the same  \nsender  or the repetitive use  of the same keywords or  something else.  \n \n \nAdvantages:  \n1. Instead  of estimating  for the entire  instance  set, local  approximations  can be \nmade  to the target  function.  \n2. This algorithm  can adapt  to new  data easily,  one which  is collected  as we  go. \n \n \n \n \n \n \n \n \n \nDisadvantages:  \n1. Classification  costs  are high \n2. Large amount of memory required to store the data, and each  \nquery involves starting the identification of a local model from scratch.  \nSome  of the  instance -based  learning  algorithms are  : \n1. K Nearest  Neighbor (KNN)  \n2. Self-Organizing  Map (SOM)  \n3. Learning  Vector  Quantization  (LVQ)  \n4. Locally  Weighted  Learning  (LWL)  Department  Of CSE MRCET  \n55  \n  \n \n \nK-Nearest  Neighbor(KNN)  Algorithm  \n\u2022 K-Nearest Neighbour is one of the simplest Machine Learning algorithms  \nbased  on Supervised  Learning  technique.  \n\u2022 K-NN algorithm  assumes  the similarity  between  the new case/data  and \navailable cases and put the new case into the category that is most similar to  \nthe available categories.  \n\u2022 K-NN algorithm stores all the available data and classifies a new data point  \nbased on the similarity. This means when new data appears then it can be  \neasily classified into  a well suite category  by using  K- NN algorithm.  \n\u2022 K-NN algorithm can be used for Regression as well as for Classification but  \nmostly  it is used for  the Classification  problems.  \n\u2022 K-NN is a non-parametric algorithm , which means it does not make any  \nassumption  on underlying  data.  \n\u2022 It is also called a lazy learner algorithm because it does not learn from the  \ntraining  set immediately  instead  it stores  the dataset  and at the time of \nclassification,  it performs an  action  on the  dataset.  \n\u2022 KNN algorithm at the training phase just stores the dataset and when it gets  \nnew data, then it classifies that data into a category that  is much similar to the  \nnew data.  \nWorking  of KNN  Algorithm  \nK-nearest  neighbours  (KNN) algorithm uses  \u2018feature similarity\u2019 to  predict  \nthe values of new data  points which further means that the new data point  \nwill be assigned a value based on how closely it matches the points in the  \ntraining  set. We can understand  its working  with the help of following  steps  \n\u2212 \nStep  1 \u2212 For implementing  any algorithm,  we need  dataset.  So during  the \nfirst step of KNN,  we must  load the  training as  well as test  data.  \nStep  2 \u2212 Next,  we need  to choose  the value  of K i.e. the nearest  data points.  \nK can  be any  integer.", "mimetype": "text/plain", "start_char_idx": 80682, "end_char_idx": 84454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b93c396-2f42-4bb5-bff3-81fdbbe6546c": {"__data__": {"id_": "2b93c396-2f42-4bb5-bff3-81fdbbe6546c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52e45947-6718-421f-bded-d83055e36d97", "node_type": "1", "metadata": {}, "hash": "fea2df02a4df0968413a1d4eda1bfb6e7a48ce0bb8d649e805e84d6ee1d8781f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5fe8ee1-25b3-4515-8dc5-f4f73f5507ab", "node_type": "1", "metadata": {}, "hash": "d548bb474ed6dcea358b77a761a90a30a2db78d27b0cfb1a6e3c344dda646eb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the nearest  data points.  \nK can  be any  integer.  \nStep  3 \u2212 For each point  in the test data do the  following  Department  Of CSE MRCET  \n56  \n  \n\u2022 3.1 \u2212 Calculate the distance between test data  and each row of training data  \nwith the help of any of the method  namely:  Euclidean,  Manhattan  or \nHamming distance. The most commonly used method to calculate distance is  \nEuclidean.  \n\u2022 3.2 \u2212 Now,  based  on the distance  value,  sort them  in ascending order.  \n\u2022 3.3 \u2212 Next,  it will choose  the top K rows from  the sorted array.  \n\u2022 3.4 \u2212 Now, it will assign a class to the test point based on most frequent class  \nof these rows.  \nStep 4  \u2013 End \nEXAMPLE  : \n \nCase  Based  Reasoning  \n \nAs we know Nearest Neighbour classifiers stores training tuples as points in  \nEuclidean  space.  But Case -Based  Reasoning  classifiers  (CBR)  use a \ndatabase of problem solutions to solve new problems. It stores the tuples or  \ncases for  problem -solving as  complex  symbolic  descriptions.  \nHow  CBR works?  \nWhen a new case arrises to classify, a Case -based Reasoner(CBR) will first  \ncheck  if an identical  training  case exists.  If one is found,  then the \naccompanying solution to that case is returned. If no identical case is found,  \nthen the CBR  will search  for training c ases having  components  that are \nsimilar to those of the new case. Conceptually, these training cases may be  \nconsidered as neighbours of the new case. If cases are represented as graphs,  \nthis involves searching for subgraphs that are similar to subgraphs wi thin the  \nnew case.  The CBR  tries to combine  the solutions  of the neighbouring  \ntraining cases to propose a solution for the new case. If compatibilities arise  \nwith the individual  solutions,  then backtracking  to search  for other  solutions  \nDepartment  Of CSE MRCET  \n57  \n  \nmay be necessary.  The CBR  may employ  background  knowledge  and \nproblem -solving strategies  to propose  a feasible  solution.  \nApplications  of CBR  includes:  \n1. Problem  resolution  for customer  service  help desks,  where  cases  describe  \nproduct -related  diagnostic  problems.  \n2. It is also applied  to areas  such as engineering  and law, where  cases  are either  \ntechnical designs  or legal  rulings,  respectively.  \n3. Medical  educations,  where  patient  case histories  and treatments  are used to \nhelp diagnose and  treat new patients.  \n \nChallenges  with  CBR  \n\u2022 Finding  a good  similarity  metric  (eg for matching  subgraphs)  and suitable  \nmethods for combining  solutions.  \n\u2022 Selecting  salient  features  for indexing  training  cases  and the development  of \nefficient  indexing  techniques.  \n \nCBR  becomes  more  intelligent  as the number  of the trade -off between  \naccuracy and efficiency evolves as the number of stored cases becomes very  \nlarge. But after a certain point, the system\u2019s efficiency will suffer as the time  \nrequired  to search  for and process  relevant  cases increases.  \n \n \nSome  differences  on eager  and lazy learning  \n\u2022 Eager  learning  methods  construct  general,  explicit  description  of the target  \nfunction  based  on the  provided  training examples.  \n\u2022 Lazy  learning  methods  simply  store  the data and generalizing  beyond  these  \ndata is postponed  until an explicit request  is made.  \n\u2022 Lazy  learning  methods  can construct  a different  approximation  to the target  \nfunction for each encountered  query instance.  \n \nLazy  learning  is very suitable  for complex  and incomplete  problem  domains,  \nwhere a complex target function can be represented by a collection of less  \ncomplex  local  approximations.  \nEager learning methods use the same approximation to the target function,  \nwhich must be learned based on training examples and before input queries  \nare observed.  Department  Of CSE MRCET  \n58  \n  \nUNIT  - IV \nPATTERN  COMPARISON  TECHNIQUES  \n \n \nPattern recognition is a process of finding regularities and similarities in data  \nusing machine learning data.", "mimetype": "text/plain", "start_char_idx": 84403, "end_char_idx": 88427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5fe8ee1-25b3-4515-8dc5-f4f73f5507ab": {"__data__": {"id_": "b5fe8ee1-25b3-4515-8dc5-f4f73f5507ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b93c396-2f42-4bb5-bff3-81fdbbe6546c", "node_type": "1", "metadata": {}, "hash": "8a283940f791ec78d7be38d01cbeb8ea0b2e9b26e3185c1255ea2484822160d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99c94d67-61af-42cc-8e2e-fae32d3ba3c6", "node_type": "1", "metadata": {}, "hash": "40e84ba2d4303f74fb358478523404bf56cb34e8aed90061a6fd48494092f925", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, these similarities can be found based on  \nstatistical  analysis,  historical  data,  or the already  gained  knowledge  by the \nmachine itself. A pattern is a regularity in the world or in abstract notions. If we  \ndiscuss sports, a description of a type would be a pattern. If a person keeps  \nwatching videos related to cricket, Y ouTube wouldn\u2019t recommend them chess  \ntutorials videos.  \nExamples: Speech recognition, speaker identification, multimedia document  \nrecognition (MDR),  automatic  medical  diagnosis.  \n \nBefore searching for a pattern there are some certain steps and the first one is to \ncollect the data from the real world. The collected data needs to be filtered and  \npreprocessed  so that its system can  extract  the features  from the data. Then  \nbased on the type of the data system will choose the appropriate algorithm  \namong  Classifica tion, Regression,  and Regression  to recognize  the pattern.  \n\u2022 Classification.  In classification,  the algorithm  assigns  labels  to data based  on \nthe predefined features.  This is an example  of supervised learning.  \n\u2022 Clustering. An algorithm splits data into a number of clusters based on the  \nsimilarity of  features.  This is an example  of unsupervised  learning.  \n\u2022 Regression. Regression algorithms try to find a relationship between variables  \nand predict unknown dependent variables based on known data. It is based on  \nsupervised learning.  [2] \n\u2022 Features  can be represented  as continuous,  discrete,  or discrete  binary  \nvariables.  A feature  is basically  a function  of one or more  measurements,  \ncomputed to quantify the significant characteristics of the object. The feature is  \none of the most  important  components  in the Pattern  Recognition  system.  \nExample: consider a football, shape, size and color, etc. are features of the  \nfootball.  Department  Of CSE MRCET  \n59  \n A feature  vector  is a set of features  that are  taken  together.  \nExample: In the above example of football, if all the features (shape, size, color  \netc.) taken  together  then the sequence  is feature  vector  ([shape,  size, color]).  \nThe feature vector is the sequence of features represented as an n -dimensional  \ncolumn  vector.  In the case of speech,  MFCC  (Mel -frequency  Cepstral  \nCoefficient) is the spectral features of the speech. The sequence of the first 13  \nfeatures forms  a feature vector.  \n \n \nTemporal  patterns  \n \n \nTemporal patterns are  one of the pattern comparison techniques that is defined  \nas a segment of signals that recurs frequently in the whole temporal signal  \nsequence . For example, the temporal signal sequences could be the movements  \nof head,  hand,  and body,  a piece of music,  and so on.  \nTemporal abstraction and data mining are two research fields that have tried to  \nsynthesis time  oriented data  and bring out  an understanding  on the hidden  \nrelationships t hat may exist between time oriented events. In clinical settings,  \nhaving the  ability  to know  the hidden  relationships  on patient data  as they \nunfold could help save a life by aiding in detection of conditions that are not  \nobvious to clinicians and healthcare workers. Understanding the hidden patterns  \nis a huge challenge due to the exponential search space unique to time -series  \ndata. In this paper, we propose a temporal pattern recognition model based on  \ndimension reduction and similarity measures th ereby maintaining the temporal  \nnature  of the raw  data \n \n \nINTRODUCTION  \nTemporal pattern processing is important  for various  intelligent  behaviours,  \nincluding hearing, vision, speech, music and motor control. Because we live in  \nan ever -changing environment, a n intelligent system, whether it be a human or a  \nrobot,  must  encode  patterns  over time,  recognize  and generate  temporal  \npatterns.  Time  is embodied  in a temporal  pattern  in two different  ways:  \u2022 \nTemporal order. It refers to the ordering among the components  of a sequence.  \nFor example,  the sequence  N-E-T is different  from  T-E-N. Temporal  order  may Department  Of CSE MRCET  \n60  \n also refer  to a syntactic  structure,  such as subject -verb-object,  where  each \ncomponent may be any  of a category  of possible symbols  \n\u2022 Time duration. Duration can play a critical role for temporal processing.", "mimetype": "text/plain", "start_char_idx": 88428, "end_char_idx": 92774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99c94d67-61af-42cc-8e2e-fae32d3ba3c6": {"__data__": {"id_": "99c94d67-61af-42cc-8e2e-fae32d3ba3c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5fe8ee1-25b3-4515-8dc5-f4f73f5507ab", "node_type": "1", "metadata": {}, "hash": "d548bb474ed6dcea358b77a761a90a30a2db78d27b0cfb1a6e3c344dda646eb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1ad92ac-3aeb-4719-9160-10c1dab00563", "node_type": "1", "metadata": {}, "hash": "4e854308e07ca7441e470f55d59c99c254522d7f9cf7f6020e671b80973c5818", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Duration can play a critical role for temporal processing. In  \nspeech recognition, for example, we want rate invariance while distinguishing  \nrelative  durations of  the vowel /i:/ (as  in beet)  and /i/ (as in  bit) \nTEMPORAL  PATTERN  RECOGNITION  \nThe shared  goal of all STM  models  is to make  input  history  available  \nsimultaneously when recognition takes place. With a  STM model in place,  \nrecognition  is not much different from  the recognition of  static  patterns.  \n \nTemplate  Matching  Using Hebbian  Learning  \nThe architecture for this type of recognition is simply a two -layer network: the  \ninput layer that incorporates STM, and the sequence recognition layer where  \neach unit encodes an individual sequence. The recognition scheme is essentially  \ntemplate  matching,  where  templates  are formed  through  following  Hebbian  \nlearning  \n \nWij(t)  = Wij(t \u20131) + C si (t)[xj  (t) \u2013 Wij(t \u20131)] \n \nwhere Wij is the connection weight from unit xj in the input layer to sequence  \nrecognizer  si in the recognition  layer.  Parameter  C controls  learning  rate. \nHebbian learning is applied after the presentation of the entire sequence is  \ncompleted. The templates thus formed can be used to recognize specific input  \nsequences. The recognition layer typically includes recurren t connections for  \nselecting a winner by self -organization (e.g. winner -take-all) during training or  \nrecognition.  \n \nAssociative  Memory  Approach  \nThe dynamics  of the Hopfield  associative  memory  model  can be characterized  \nas evolving  towards  the memory  state most  similar  to the current  input  pattern.  Department  Of CSE MRCET  \n61  \n If one views each memory state as a category, the Hopfield net performs pattern  \nrecognition: the recalled category is the recognized pattern. This process of  \ndynam ic evolution  can also be viewed  as an optimization  process,  which  \nminimizes a  cost function  until equilibrium  is reached.  \nWith  normalized  exponential  kernel  STM,  Tank  and Hopfield  (1987)  described  \na recognition  network  based  on associative  memory  dynamics.  A layer  of \nsequence recognizers receives inputs from the STM model. Each recognizer  \nencodes a different template sequence by its unique weight vector acting upon  \nthe inputs in STM. In addition, recognizers form a competitive network. The  \nrecognition  process  uses the current  input  sequence  (evidence)  to bias a \nminimization process so that the most similar template wins the competition,  \nthus activating  its corresponding  recognizer. Due to  the exponential  kernels,  \nthey demonstrated  that recognition  is fairly robust  to time warping,  distortions  \nin duration. A similar architecture is later applied to speakerindependent spoken  \ndigit recognition.  \n \nMultilayer  Perceptrons  \nA popular  approach  to temporal  pattern  learning  is multilayer  perceptrons  \n(MLP).  MLPs  have  been  demonstrated  to be effective  for static  pattern  \nrecognition. It is natural to combine MLP with an STM model to do temporal  \npattern recognition. For example, using delay line STM Waibel et al. (1989)  \nreported  an architecture  called  Time  Delay  Neural  Networks  (TDNN)  for \nspoken phoneme recognition. Besides the input layer, TDNN uses 2 hidden  \nlayers and an output layer where each unit encodes one phoneme. The feed  \nforward connections converge from the input layer to each successive layer so  \nthat each unit  in a specific layer receives inputs within a limited time window  \nfrom the previous layer. They demonstrated good recognition performance: for  \nthe three stop consonants /b/, /d/, and /g/, the accuracy of speaker dependent  \nrecognition reached  98.5%.  \nDYNAMIC  TIME  WARPING  \nSounds like time traveling or some kind of future technic, however, it is not.  \nDynamic  Time  Warping  is used  to compare  the similarity  or calculate  the Department  Of CSE MRCET  \n62  \n \ndistance  between  two arrays  or time series  with different  length.", "mimetype": "text/plain", "start_char_idx": 92716, "end_char_idx": 96731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1ad92ac-3aeb-4719-9160-10c1dab00563": {"__data__": {"id_": "e1ad92ac-3aeb-4719-9160-10c1dab00563", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99c94d67-61af-42cc-8e2e-fae32d3ba3c6", "node_type": "1", "metadata": {}, "hash": "40e84ba2d4303f74fb358478523404bf56cb34e8aed90061a6fd48494092f925", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a9dbf37-a9b7-499d-9d97-815d3de50983", "node_type": "1", "metadata": {}, "hash": "b2ec50414138946e2a9cbd2d6e5ec91865a28c3300c0bf518f5ee2d3006883e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Suppose  we \nwant  to calculate the  distance  of two equal -length  arrays:  \na = [1, 2, \n3] b = [3, \n2, 2] \nHow to do that? One obvious way is to match up a and b in 1 -to-1 fashion and  \nsum up the total distance  of each component.  This sounds  easy,  but what  if a \nand b  have different  lengths?  \na = [1, 2, 3] b \n= [2, 2, 2, 3, \n4] \nHow to match them up? Which should map to which? To solve the problem,  \nthere comes dynamic time warping. Just as its name indicates, to warp the series  \nso that they can match  up. \n \nUse Cases  \nBefore digging into the algorithm, you might have the question that is it useful?  \nDo we really need to compare the distance between two unequal -length time  \nseries?  \nYes, in a lot of scenarios DTW  is playing a  key role.  \nSound  Pattern  Recognition  \nOne use case is to detect  the sound  pattern  of the same  kind.  Suppose  we want  \nto recognise the voice of a person by analysing his sound track, and we are able  \nto collect his sound track of saying Hello in one scenario. However, people  \nspeak in the same word in different ways, what if he speaks hello in a much  \nslower pace like Heeeeeeelloooooo , we will need an algorithm to match up the  \nsound track of different lengths and be able to identify they come from the same  \nperson.  Department  Of CSE MRCET  \n63  \n \n \n \n \n \n \n \n \n \n \nStock  Market  \nIn a stock market, people always hope to be able to predict the future, however  \nusing general machine learning algorithms can be exhaustive, as most prediction  \ntask requires  test and training  set to have  the same  dimension  of features.  \nHowever, if you ever speculate in the stock market, you will know that even the  \nsame pattern of a stock can have very different length reflection on klines and  \nindicators.  \nDepartment  Of CSE MRCET  \n64  \n  \n \n \n \n \nIn time series  analysis,  dynamic  time warping  (DTW)  is one of the algorithms  \nfor measuring similarity between two temporal sequences, which may vary in  \nspeed. DTW has been applied to temporal sequences of video, audio, and  \ngraphics data \u2014 indeed, any data that can be turned into a linear sequence can  \nbe analysed  with DTW.  \nThe idea to compare arrays with different length is to build one -to-many and  \nmany -to-one matches so that the total distance can be minimised between the  \ntwo. \n \n \n \n \nSuppose  we have  two different  arrays  red and blue with different  length:  \nDepartment  Of CSE MRCET  \n65  \n  \n \n \n \n \nClearly  these  two series  follow  the same  pattern,  but the blue curve  is longer  \nthan the red. If we apply the one -to-one match, shown in the top, the mapping is  \nnot perfectly  synced  up and  the tail of  the blue curve  is being  left out.  \n \nDTW overcomes the issue by developing a one -to-many match so that the  \ntroughs and peaks with the same pattern  are perfectly matched, and there is no  \nleft out  for both curves(shown  in the bottom top).  \nDepartment  Of CSE MRCET  \n66  \n l > \nk Rules  \nIn general, DTW is a method that calculates an optimal match between  two \ngiven sequences (e.g. time series) w ith certain restriction and rules(comes from  \nwiki):  \n \n\u2022 Every  index  from  the first sequence  must  be matched  with one or more  indices  \nfrom  the other sequence and  vice versa  \n \n\u2022 The first index from the first sequence must be matched with the first index from  \nthe other sequence  (but it does not have  to be its  only match)  \n\u2022 The last index  from  the first sequence  must  be matched  with the last index  from  \nthe other sequence  (but it does not have  to be its  only match)  \n\u2022 The mapping  of the indices  from  the first sequence  to indices  from  the other  \nsequence  must  be monotonically  increasing,  and vice versa,  i.e.", "mimetype": "text/plain", "start_char_idx": 96733, "end_char_idx": 100488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a9dbf37-a9b7-499d-9d97-815d3de50983": {"__data__": {"id_": "9a9dbf37-a9b7-499d-9d97-815d3de50983", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1ad92ac-3aeb-4719-9160-10c1dab00563", "node_type": "1", "metadata": {}, "hash": "4e854308e07ca7441e470f55d59c99c254522d7f9cf7f6020e671b80973c5818", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5aa4b107-4757-47a2-a11e-9605c5f3da1f", "node_type": "1", "metadata": {}, "hash": "2355d37443c61d21fdb3153ac608b8e591a46941866e5874fd452b41555679f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "if \nfrom  the first  sequence,  then are indices  \nthere  must not  be two indices  in the other  sequence,  such \n \n \nthat index i is matched with index  l and index j is matched with index  k , and  \nvice versa.  \nThe optimal match is denoted by the match that satisfies all the restrictions and  \nthe rules and  that has the minimal cost, where the cost is computed as the sum of  \nabsolute  differences,  for each matched pair  of indices,  between their  values.  j > i Department  of CSE MRCET  \n67  \n Introduction to  Clustering : \nIt is basically  a type of unsupervise d learnin g method . An unsupervised  learning  method  \nis a method in which we draw references from datasets consisting of input data without  \nlabelled  responses.  Generally,  it is used as a process  to find meaningful  structure,  \nexplanatory underlying processes, generative features, and groupings inherent in a set of  \nexamples.  \nClustering is the task of dividing the population or data points into a number of groups  \nsuch that data points in the same groups are more similar to other data points in the same  \ngroup and dissimilar to the data points in other groups. It is basically a collection of  \nobjects on the basis  of similarity  and dissimilarity between  them.  \nFor ex \u2013 The data points in the graph below clustered together can be classified into one  \nsingle group. We can  distinguish the clusters, and we can identify that there are 3 clusters  \nin the below  picture.  \n \n \nIt is not  necessary  for clusters  to be spherical.  Such  as: \nDepartment  of CSE MRCET  \n68  \n  \n \n \n \n \n \n \n \nDBSCAN:  Density -based  Spatial  Clustering  of Applications  with  Noise  \nThese data points are clustered by using the basic concept that the data point lies within  \nthe given constraint from the cluster center. Various distance methods and techniques are  \nused for the calculation  of the  outliers.  \nWhy  Clustering?  \nClustering is very much important as it determines the intrinsic grouping among the  \nunlabelled data present. There are no criteria for goo d clustering. It depends on the user,  \nwhat is the criteria they may use which satisfy their need. For instance, we could be  \ninterested in finding representatives for homogeneous groups (data reduction), in finding  \n\u201cnatural clusters\u201d and describe their unkn own properties (\u201cnatural\u201d data types), in finding  \nuseful and suitable groupings (\u201cuseful\u201d data classes) or in finding unusual data objects  \n(outlier  detection).  This algorithm  must  make  some  assumptions  that constitute  the \nsimilarity  of points and  each assumption  make  different and  equally valid  clusters.  \nClustering  Methods  : \n\u2022 Density -Based Methods: These methods consider the clusters as the dense region having  \nsome  similarities  and differences  from  the lower  dense  region  of the space.  These  \nmethods have good accuracy and the ability to merge two clusters. Example DBSCAN  \n(Density -Based Spatial Clustering of Applications with Noise) , OPTICS (Ordering Points  \nto Identify Clustering  Structure) , etc. \n\u2022 Hierarchical  Based  Methods:  The clusters  formed  in this method  form  a treetype  \nstructure based on the hierarchy. New clusters are formed using the previously formed  \none. It is divided  into two category  \n\u2022 Agglomerative  (bottom -up approach ) \n\u2022 Divisive  (top-down  approach ) \nDepartment  of CSE MRCET  \n69  \n examples  CURE  (Clustering  Using  Representatives),  BIRCH  (Balanced  Iterative  \nReducing Clustering  and using  Hierarchies) , etc. \n\u2022 Partitioning  Methods: These methods partition the objects into k clusters and each  \npartition  forms  one cluster.  This method  is used to optimize  an objective  criterion  \nsimilarity function such as when the distance is a major parameter example K-means,  \nCLARANS  (Clustering Large  Applications  based upon  Randomized  Search) , etc. \n\u2022 Grid -based Methods: In this method, the data space is formulated into a finite number of  \ncells that form a grid -like structure. All the clustering operations done on these grids are  \nfast and independent  of the number  of data objects  example  STING  (Statisti cal \nInformation Grid),  wave cluster,  CLIQUE  (CLustering In Quest) , etc.", "mimetype": "text/plain", "start_char_idx": 100489, "end_char_idx": 104728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5aa4b107-4757-47a2-a11e-9605c5f3da1f": {"__data__": {"id_": "5aa4b107-4757-47a2-a11e-9605c5f3da1f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a9dbf37-a9b7-499d-9d97-815d3de50983", "node_type": "1", "metadata": {}, "hash": "b2ec50414138946e2a9cbd2d6e5ec91865a28c3300c0bf518f5ee2d3006883e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6faa1076-7c26-4e9e-a75c-a29c27698bbf", "node_type": "1", "metadata": {}, "hash": "460496fc53a4a5ef476a897005a60e51d37d6fa561bf140424726d912fcaeca9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K means  Clustering:  \nIt is the simplest unsupervised learning algorithm that solves clustering problem.K -means  \nalgorithm partitions n observations into k clusters where each observation belongs to the  \ncluster  with the nearest  mean  serving as a  prototype of  the cluster.  \n \nApplications  of Clustering  in different  fields  \n\u2022 Marketing: It can be used to characterize & discover customer segments for marketing  \npurposes.  \n\u2022 Biology:  It can be used for classification  among  different  species  of plants  and animals.  \n\u2022 Libraries:  It is used in clustering  different  books  on the basis  of topics  and information.  \n\u2022 Insurance: It is used to acknowledge the customers, their policies and identifying the  \nfrauds.  \n\u2022 City Planning: It is used to make  groups  of houses  and to study  their values  based on  \ntheir geographical  locations and  other  factors  present.  \nDepartment  of CSE MRCET  \n70  \n \u2022 Earthquake studies: By learning the earthquake -affected areas we can determine the  \ndangerous  zones.  \nThe algorithm will categorize the items into k groups of similarity. To calculate  \nthat similarity, we will use the euclidean distance as measurement. The algorithm  \nworks as follows:  \n \n1. First,  we initialize  k points,  called  means,  randomly.  \n2. We categorize each item to its closest mean and we update the mean\u2019s coordinates, which  \nare the averages  of the items  categorized  in that mean  so far. \n3. We repeat the process for a given number of iterations and at the end, we have our  \nclusters.  \nThe \u201cpoints\u201d mentioned above are called means because they hold the mean values of the  \nitems categorized in them. To initialize these means, we have a lot of options. An intuitive  \nmethod is to initialize the means at random items in the data set. Another method is to  \ninitialize the means at random values between the boundaries of the data set (if for a  \nfeature x the items have values in [0,3], we will initialize the means with values for x at \n[0,3]).  \n \n \n \nThe above  algor ithm in pseudocode:  \n \n \n \n \nK-MODE  CLUSTERING  \nKModes clustering is one of the unsupervised Machine Learning algorithms that is used to  \ncluster  categorical  variables.  \nHow  does the KModes  algorithm  work?  \n \n1. Pick K observations  at random  and use  them  as leaders/clusters  \n2. Calculate  the dissimilarities  and assign  each observation  to its closest  cluster  \n3. Define  new modes  for the clusters  \nDepartment  of CSE MRCET  \n71  \n 4. Repeat  2\u20133 steps  until there  are is no re-assignment  required  \nExample:  Imagine we have a dataset that has the information about hair color, eye color, and  \nskin color of persons. We aim to group them based on the available information(maybe we  \nwant  to suggest  some  styling  ideas)  \nHair color, eye color, and skin color are all categorical variables. Below  is how our dataset  \nlooks  like. \n \n \nAlright, we have the sample data now. Let us proceed by defining the number of  \nclusters(K)=3  \nStep 1: Pick K observations at random and use them as leaders/clusters  \nI am choosing P1,  P7, P8 as leaders/clusters  \n \n \n \n \n \n \n \n \n \n \n \n \nStep 2: Calculate the dissimilarities(no. of mismatches) and assign each observation to its  \nclosest cluster  \nIteratively compare the cluster data points to each of the observations. Similar data points  \ngive 0, dissimilar  data points  give 1.  \nDepartment  of CSE MRCET  \n72  \n  \n \n \nComparing  leader/Cluster  P1 to the observation  P1 gives  0 dissimilarities  \n \nComparing  leader/cluster  P1 to the observation  P2 gives  3(1+1+1)  \ndissimilarities. Likewise, calculate all the dissimilarities and put them in a matrix as shown  \nbelow  and assign  the observations  to their closest  cluster  (cluster  that has the least \ndissimilarity)  \nDepartment  of CSE MRCET  \n73  \n After step 2, the observations P1, P2, P5 are assigned to cluster 1; P3, P7 are assigned to  \nCluster  2; and P4, P6, P8 are  assigned  to cluster 3.", "mimetype": "text/plain", "start_char_idx": 104738, "end_char_idx": 108728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6faa1076-7c26-4e9e-a75c-a29c27698bbf": {"__data__": {"id_": "6faa1076-7c26-4e9e-a75c-a29c27698bbf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5aa4b107-4757-47a2-a11e-9605c5f3da1f", "node_type": "1", "metadata": {}, "hash": "2355d37443c61d21fdb3153ac608b8e591a46941866e5874fd452b41555679f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cb186a8-5a82-43af-a85a-3d7c9fb498cf", "node_type": "1", "metadata": {}, "hash": "3a3f1c5d5c446a293ef2e5cf24b1c45c6314d6c03d5cdf1a32d6d8231fd0dd8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Step 3: Define  new modes  for the clusters  \nMode is simply the most observed value. Mark the observations according to the cluster  \nthey belong to. Observations of Cluster 1 are marked in Yellow, Cluster 2 are marked in  \nBrick red,  and Cluster 3  are marked  in Purple.  \n \nConsidering one cluster at a time, for each feature, look for the Mode and update the new  \nleaders.  \n \nExplanation: Cluster 1 observations( P1, P2, P5) has brunette as the most observed hair  \ncolor,  amber  as the  most  observed  eye color,  and fair as the  most  observed skin color.  \nBelow  are our new leaders after  the update.  \n \n \nRepeat steps 2 \u20134 : After obtaining the new leaders, again calculate the  dissimilarities  \nbetween  the observations  and the newly  obtained leaders.  \nDepartment  of CSE MRCET  \n74  \n  \n \n \nComparing  Cluster  1 to the observation  P1 gives  1 dissimilarity.  \n \n \n \nComparing  Cluster  1 to the observation  P2 gives  2 dissimilarities.  \n \nLikewise,  calculate  all the dissimilarities  and put them  in a matrix.  Assign  each \nobservation to  its closest  cluster.  \nDepartment  of CSE MRCET  \n75  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe observations  P1, P2, P5 are assigned  to Cluster  1; P3, P7 are assigned  to Cluster  2; \nand P4, P6,  P8 are assigned  to Cluster  3. \n \n \n \n \nWe stop here as we see there is no change in the assignment of observations.  \nImplementation  of KModes  in Python:  \nBegin  with Importing  necessary  libraries  Department  of CSE MRCET  \n76  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nVector  Quantization  \nLearning Vector Quantization ( or LVQ ) is a type of Artificial Neural Network which  \nalso inspired by biological models of neural systems. It is based on prototype supervised  \nlearning classification algorithm and trained its network through a competitive learning  \nalgorithm  similar  to Self Organizing  Map.  It can also deal with the multiclass  \nclassification problem. LVQ has two layers, one is the Input layer and the other one is the  \nOutpu t layer. The architecture of the Learning Vector Quantization with the number of  \nclasses  in an input data  and n number  of input features  for any sample  is given below:  Department  of CSE MRCET  \n77  \n i  \n \n \n \n \nLet say an input data of size ( m, n ) where m is number of training example and n is the  \nnumber of features in each example and a label vector of size ( m, 1 ). First, it initializes  \nthe weights of size ( n, c ) from the first c number of training samples with different labels  \nand should be discarded from all trai ning samples. Here, c is the number of classes. Then  \niterate over the remaining input data, for each training example, it updates the winning  \nvector ( weight vector with the shortest distance ( e.g Euclidean distance ) from training  \nexample  ). Weight  updat ion rule is given  by : \nwij = wij(old)  - alpha(t)  * (x k - wij(old))  \n \nwhere alpha is a learning rate at time t, j denotes the winning vector, i denotes the ith \nfeature of training example and k denotes the kth training example from the input data.  \nAfter training the LVQ network, trained weights are used for classifying new examples.  \nA new  example  labeled  with the class  of winning vector.  \n \nAlgorithm  \n \nSteps  involved are  : \n\u2022 Weight  initialization  \n\u2022 For 1  to N number  of epochs  \n\u2022 Select  a training  example  \n\u2022 Compute  the winning  vector  \n\u2022 Update  the winning  vector  \n\u2022 Repeat  steps  3, 4, 5 for all training  example.  \n\u2022 Classify  test sample  \nDepartment  of CSE MRCET   \n  \n \nGenetic  Algorithms  UNIT - V \n \n \nGenetic Algorithms(GAs)  are adaptive heuristic search  algorithms  that \nbelong  to the larger  part of evolutionary  algorithms.  Genetic  algorithms  \nare based  on the ideas  of natural  selection  and genetics.", "mimetype": "text/plain", "start_char_idx": 108731, "end_char_idx": 112559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7cb186a8-5a82-43af-a85a-3d7c9fb498cf": {"__data__": {"id_": "7cb186a8-5a82-43af-a85a-3d7c9fb498cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6faa1076-7c26-4e9e-a75c-a29c27698bbf", "node_type": "1", "metadata": {}, "hash": "460496fc53a4a5ef476a897005a60e51d37d6fa561bf140424726d912fcaeca9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6f75010-c249-4c13-b50b-072cee603031", "node_type": "1", "metadata": {}, "hash": "d47b57521d18884f710e35c69bac3d671d6a98d6c46a78b62d43f1436ec254a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These  are \nintelligent  exploitation of random search provided with historical data to  \ndirect the search into the region of better performance in solution space.  \nThey  are commonly  used to generate  high-quality  solutions  for \noptimization  problems  and search  problems.  \nGenetic algorithms simulate the process of natural selection which means  \nthose species who can adapt to changes in their environment are able to  \nsurvive and reproduce and go to next generation. In simple words, they  \nsimulate  \u201csurvival  of the fittest\u201d  among  individ ual of consecutive  \ngeneration for solving a problem. Each generation consist of a population  \nof individuals and each individual represents a point in search space and  \npossible  solution.  Each  individual  is represented  as a string  of \ncharacter/integer/float/ bits. This string  is analogous  to the Chromosome.  \nDifferent  search  methods for  induction  \nIn the field of  machine learning , an induction  algorithm  represents an  \nexample  of using  mathematical  principles  for the development  of \nsoph isticated computing systems. Machine learning systems go beyond a  \nsimple \u201crote input/output \u201d funct ion, and evolve the results that they supply  \nwith continued  use. Induction  algorithms  can help with the real-time \nhandling of  sophisticated  data sets, or more long -term efforts.  \n \nThe induction algorithm is something that applies to systems that show  \ncomplex results depending on what they are set up for. One of the most  \nfundamental ways that engineers use an induction algorithm is to enhance  \nknowledge  acquisition  in a given  system.  In other  words,  with the \nalgorithm  in place,  the set of \u201cknowledge  data\u201d that end users  get is \nsomehow  improved, whether that\u2019s  regarding  the quantity  of data, the  \nfiltering of noise and undesirable results, or the refinement of some data  \npoints.  \nMachine  Learning  R20D5803  Department  of CSE MRCET  \n79  \n Although the technical descriptions of induction algorithms are largely the  \nterritory of mathematical and scientific journals, one of the basic ideas  \nabout using the induction algorithm is that it can organize \u201cclassification  \nrules\u201d according to the induction principle and separate corollary  results  \nfrom  different  kinds  of \n \nsystem  noise  or exceptions.  Filtering  out noise  from  a domain  is a \nprominent use of the induction algorithm in general. There is the idea that  \nin real-world data filtering , induction  algorithms  can compose  different  \nsets of rules for both the legitimate results and the system noise, in order to  \ndistinguish one from  the other.  \n \nBy setting up induction algorithms according to certain training examples,  \nstakeholders are looking for the ability of these systems to identify and  \nassess consistent rules and data  that represents exceptions to these rules. In  \na sense, the use of an induction algorithm uses the induction principle to  \n\u201cprove\u201d certain results that can aid knowledge, because they provide more  \nmarked delineations in a data set (or multiple data sets) \u2013 distinctions that  \ncan drive all  sorts of  end user capabilities.  \n \nLike other kinds of machine learning software, induction algorithms are  \noften  thought  of as a form  of \u201cdecision  support.\u201d  \n \n\u201cWe consider the principal task of a real -world induction system to be  \nassisting the expert in expressing his or her expertise,\u201d write the authors of  \na Turing Institute paper on induction in machine learning back in the  \n1980s.  \u201cConsequ ently,  we require  that the induced  rules  are highly  \npredictive  and are  easily comprehensible  to the expert.\u201d  \n \nWith this in mind, induction algorithms can be part of many kinds of  \nsoftware products that seek to refine data and produce evolving results for  \nhuman  users.  In general,  machine  learning  and the use of visual  \ndashboards  is generating new t ools through which users can more rapidly  \ndevelop  in-depth  knowledge  about  any given  system,  whether  it's related  \nto marine research, medical diagnosis, e-commerce , or any other kind of  \ndata-rich system.  \n \nExplanation -Based  Learning  (EBL)  Department  of CSE MRCET  \n80  \n In simple terms, it is the ability to gain basic problem -solving techniques  \nby observing and analysing solutions to specific problems. In terms of  \nMachine Learning, it is an algorithm that aims to understand why an  \nexample is a part of a particular concept to make generalizations or form  \nconcepts  from  training  examples.", "mimetype": "text/plain", "start_char_idx": 112561, "end_char_idx": 117111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6f75010-c249-4c13-b50b-072cee603031": {"__data__": {"id_": "c6f75010-c249-4c13-b50b-072cee603031", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cb186a8-5a82-43af-a85a-3d7c9fb498cf", "node_type": "1", "metadata": {}, "hash": "3a3f1c5d5c446a293ef2e5cf24b1c45c6314d6c03d5cdf1a32d6d8231fd0dd8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dac663a-04cb-4da4-b383-37a12eb6d083", "node_type": "1", "metadata": {}, "hash": "8b5d21a3951902d63d094d9fc1f9b4fd51ecd0d6e831eed019668e74040765b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example,  EBL  uses a domain  \ntheory and creates a program that learns to play chess.  EBL involves 2  \nsteps:  \n1. Explanation \u2014 The domain theory is used to eliminate all the unimportant  \ntraining example while retaining the important ones that best describe the goal  \nconcep t. \n2. Generalization  \u2014 The explanation  of the goal concept  is made  as general  \nand widely applicable as possible. This ensures that all cases are covered,  \nnot just  certain  specific ones.  \nEBL  Architecture:  \n\u2022 EBL  model  during  training  \n\u2022 During training, the model generalizes the training example in such a way that  \nall scenarios lead to the Goal Concept, not just in specific cases. (As shown in  \nFig 1)  Department  of CSE MRCET  \n81  \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\u2022 EBL  model  after  training  \n\u2022 Post training, EBL model tends to directly reach the hypothesis space involving  \nthe goal concept.  (As shown in Fig  2) \nDepartment  of CSE MRCET  \n82  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nDimensionality  Reduction  \nAn intuitive example of dimensionality reduction can be discussed through  \na simple e -mail classification problem, where we need to classify whether  \nthe e-mail is spam  or not. This can involve  a large  number  of features,  \nsuch as whether or not the e -mail has a generic title, the content of the e - \nmail, whether the e -mail uses a tem plate, etc. However, some of these  \nfeatures may overlap. In another condition, a classification problem that  \nrelies  on both humidity  and rainfall  can be collapsed  into just one \nunderlying feature, since both of the aforementioned are correlated to a  \nhigh degree.  Hence,  we can reduce  the number  of features  in such  \nproblems. A 3D classification problem can be hard to visualize, whereas a  \n2-D one can  be mapped  to a simple 2  dimensional  space, and  a 1-D \nproblem to a simple line. The below figure illustrates this concept, where a  \n3-D feature space is split into two 1 -D feature spaces, and later, if found to  \nbe correlated,  the number  of features  can be reduced  even further.  Department  of CSE MRCET  \n83  \n  \n \n \n \n \nComponents  of Dimensionality  \nReduction There are two components of dimensionality  \nreduction:  \n\u2022 Feature selection: In this, we try to find a subset of the original set of variables,  \nor features, to get a smaller subset which can be used to model the problem. It  \nusually involves  three ways:  \n1. Filter  \n2. Wrapper  \n3. Embedded  \n\u2022 Feature  extraction:  This reduces  the data in a high dimensional  space  to a \nlower  dimension space,  i.e. a space  with lesser  no. of dimensions.  \nMethods  of Dimensionality  Reduction  The various  \nmethods  used for dimensionality  reduction  include:  \n\u2022 Principal  Component  Analysis  (PCA)  \n\u2022 Linear  Discriminant  Analysis  (LDA)  \n\u2022 Generalized  Discriminant  Analysis  (GDA)  \nDimensionality  reduction  may be both linear  or non-linear,  depending  \nupon  the method  used.  The prime  linear  method,  called  Principal  \nComponent Analysis,  or PCA,  is discussed  below.  \nDepartment  of CSE MRCET  \n84  \n  \nPrincipal  Component  Analysis  \nThis method was introduced by Karl Pearson. It works on a condition that  \nwhile the data in a higher dimensional space is mapped to data in a lower  \ndimension space, the variance of the data in the lower dimensional space  \nshould be maximum.  \n \nIt involves  the following  steps:  \n\u2022 Construct  the covariance  matrix  of the data.  \n\u2022 Compute  the eigenvectors  of this matrix.  \n\u2022 Eigenvectors corresponding to the largest eigenvalues are used to reconstruct a  \nlarge  fraction  of variance  of the  original  data.  \nHence, we are left with a lesser number of eigenvectors, and there might  \nhave been some data loss in the process. But, the most important variances  \nshould be retained by  the remaining  eigenvectors.  \nAdvantages  of Dimensionality  Reduction  \n\u2022 It helps  in data compression,  and hence  reduced  storage  space.  \n\u2022 It reduces  computation  time.", "mimetype": "text/plain", "start_char_idx": 117113, "end_char_idx": 121153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dac663a-04cb-4da4-b383-37a12eb6d083": {"__data__": {"id_": "3dac663a-04cb-4da4-b383-37a12eb6d083", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6f75010-c249-4c13-b50b-072cee603031", "node_type": "1", "metadata": {}, "hash": "d47b57521d18884f710e35c69bac3d671d6a98d6c46a78b62d43f1436ec254a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7422bbd9-800e-4f59-8a85-667a600f69a2", "node_type": "1", "metadata": {}, "hash": "4f54bf3950cd71186e73b2173680b84f264e2efdb2a655420b632c4ed36eb067", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 It reduces  computation  time.  \n\u2022 It also helps  remove  redundant  features,  if any. Disadvantages  of \nDimensionality  Reduction  \u2022 It may lead  to some amount of  data loss.  \n\u2022 PCA  tends  to find linear  correlations  between  variables,  which  is sometimes  \nundesirable.  \n\u2022 PCA  fails in cases  where  mean  and covariance  are not enough  to define  \ndatasets.  \nDepartment  of CSE MRCET  \n85  \n \u2022 We may not know  how many  principal  components  to keep - in practice,  some  \nthumb rules  are applied.  \n \n \n \nFactor  analysis . \nFactor analysis is a statistical  method used to describe variability  among  \nobserved,  correlated  variables  in terms  of a potentially  lower  number  of \nobserved variables called factors . For example, it is possible that variations in  \nsix observe d variables  mainly  reflect  the variations  in two unobserved  \n(underlying) variables. Factor analysis searches for such joint variations in  \nresponse to unobserved latent variables. The observed  variables are modelled  \nas linear combinations of the potential factors plus \"error\" terms, hence factor  \nanalysis can  be thought  of as a special  case of errors -invariables models . \n \nHere,There is a party going into a room full of people. There is \u2018n\u2019 number of  \nspeakers in that room and they are speaking simultaneously at the party. In the  \nsame  room,  there  are also \u2018n\u2019 number  of microphones  placed  at different  \nDepartment  of CSE MRCET  \n86  \n distances from the speakers which are recording \u2018n\u2019 speakers\u2019 voice signals.  \nHence, the number of speakers is equal to the number must of microphones in \nthe room.  \nNow, using these microphones\u2019 recordings,  we want to separate all the \u2018n\u2019  \nspeakers\u2019 voice signals in the room given each microphone recorded the voice  \nsignals coming from each speaker of different intensity due to the difference in  \ndistances between them. Decomposing the mixed signal of each microphone\u2019s  \nrecording into independent source\u2019s speech signal can be done by using the  \nmachine  learning  technique,  independent  component analysis.  \n[ X1, X2, \u2026.., Xn ] => [  Y1, Y2,  \u2026.., Yn ] \nwhere, X1, X2,  \u2026, Xn are the original signals present in the mixed signal and  \nY1, Y2, \u2026, Yn are the new features and are independent components which are  \nindependent of each  other.  \n \n \n \nRestrictions  on ICA \n \n1. The independent  components  generated  by the ICA are assumed  to be \nstatistically independent  of each  other.  \n2. The independent components generated by the ICA must have non -gaussian  \ndistribution.  \n3. The number  of independent  components  generated  by the ICA is equal  to the \nnumber  of observed  mixtures.  \n \nMultidimensional  scaling  \nMultidimensional  scaling  is a visual  representation  of distances  or \ndissimilarities between sets  of objects . \n\u201cObjects\u201d  can be colors,  faces,  map coordinates,  political  persuasion,  or any \nkind of real  or conceptual  stimuli  \n(Kruskal  and Wish,  1978).  Objects  that are more  similar  (or have  shorter  \ndistances) are closer together on the graph than objects that are less similar (or  \nhave  longer  distances).  As well as interpreting  dissimilarities  as distances  on a Department  of CSE MRCET  \n87  \n graph,  MDS  can also serve  as a dimension  reduction  technique  for high- \ndimensional data (Buja et. al,  2007).  \nThe term scaling  comes  from  psychometrics , where  abstract  concepts  \n(\u201cobjects\u201d)  are assigned  numbers  according  to a rule (Trochim,  2006).  For \nexample, you may want to quantify a person\u2019s attitude to global warming. You  \ncould a ssign a \u201c1\u201d to \u201cdoesn\u2019t believe in global warming\u201d, a 10 to \u201cfirmly  \nbelieves in global warming\u201d and a scale of 2 to 9 for attitudes in between. You  \ncan also think of \u201cscaling\u201d as the fact that you\u2019re essentially scaling down the  \ndata (i.e. \nmaking it simple r by creating lower -dimensional data).", "mimetype": "text/plain", "start_char_idx": 121121, "end_char_idx": 125037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7422bbd9-800e-4f59-8a85-667a600f69a2": {"__data__": {"id_": "7422bbd9-800e-4f59-8a85-667a600f69a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "252572b0-4d98-4297-adf5-e903d3caab61", "node_type": "4", "metadata": {}, "hash": "c53c3acb9a6716c1d3fc0a248575904cfde9906bfd0bd94d96352d2a0476c2e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dac663a-04cb-4da4-b383-37a12eb6d083", "node_type": "1", "metadata": {}, "hash": "8b5d21a3951902d63d094d9fc1f9b4fd51ecd0d6e831eed019668e74040765b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "making it simple r by creating lower -dimensional data). Data that is scaled down  \nin dimension keeps similar properties. For example, two data points that are  \nclose together in high-dimensional  space will also be close together in low - \ndimensional space (Martinez, 2005). The \u201c multidimensional \u201d part is du e to the  \nfact that you aren\u2019t  limited  to two dimensional  graphs  or data.  Three - \ndimensional,  four-dimensional and  higher  plots  are possible.  \nMDS is now used over a wide variety of disciplines. It\u2019s use isn\u2019t limited to a  \nspecific matrix or set of data; In fact, just about any matrix can be analyzed with  \nthe technique  as long as the matrix  contains  some  type of relational  data \n(Young,  2013).  Examples  of relational  data include  correlations , distances,  \nmultiple  rating  scales  or similar ities.  \nManifold  learning  \n \nWhat  is a manifold?  \n \nA two -dimensional manifold is any 2 -D shape that can be made to fit in a higher  \ndimensional space  by twisting or  bending  it, loosely  speaking.  Department  of CSE MRCET  \n88  \n What  is the Manifold  Hypothesis?  \n \n\u201cThe Manifold Hypothesis states that real -world high -dimensional data lie on  \nlow dimensional  manifolds  embedded  within  the high-dimensional  space.\u201d  \nIn simpler terms, it means that higher -dimensional data most of the time lies on  \na much closer lower -dimensional manifold. The process of modelling the  \nmanifold  on which training  instances  lie is called  Manifold  Learning . \nLocally  Linear  Embedding  (LLE)  \n \nLocally Linear Embedding (LLE) is a Manifold Learning technique that is used  \nfor non-linear  dimensionality  reduction.  It is an unsupervised  learning  algorithm  \nthat produces low -dimensional embeddings of high -dimensional inputs, relating  \neach training  instance  to its  closest  neighbor.  \nHow  does LLE work?  \n \nFor each training instance x(i), the algorithm first finds its k nearest neighbors  \nand then tries to express x(i) as a linear function of them. In general, if there are  \nm training instances in total, then it tries to find the set of weights w which  \nminimizes  the squared  distance  between  x(i) and its linear  representation.  \n \nSo, the cost function  is given  by \n \n \nwhere  wi,j =0, if j is not included  in the k closest neighbors  of i. \n \nAlso,  it normalizes  the weights  for each training  instance  x(i), \nDepartment  of CSE MRCET  \n89  \n  \n \n \nFinally, each high -dimensional training instance x(i) is mapped to a low - \ndimensional (say, d dimensions) vector y(i) while preserving the neighborhood  \nrelationships. This is done by choosing d -dimensional coordinates which  \nminimize  the cost  function,  \n \nHere the weights  wi,j are kept fixed  while  we try  to find the optimum coordinates  \ny(i)", "mimetype": "text/plain", "start_char_idx": 124981, "end_char_idx": 127785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"252572b0-4d98-4297-adf5-e903d3caab61": {"node_ids": ["df91900d-c71f-4c87-bcc4-da9d367b0869", "37300d2a-505a-4ef9-8893-62467eb29d69", "af4915f1-bada-411c-9181-9ba72ada4879", "c5159961-801d-4b39-8033-8ab03b9e19ed", "be57c1d0-0221-4895-b221-6d155adf0abe", "8d1b2091-f2e9-4ac5-8e9b-1fe1b7c3bb44", "9aeb9d5f-542b-4697-b814-d5506275ed3c", "df1575ec-5690-4190-bbb9-1adb89cdbe46", "570f1ecd-88ad-472c-b0c5-f1890359ac45", "53c03067-c46c-4d89-a622-e31d357a40db", "2ef7482e-55a8-4960-b0de-62afa976c5ec", "3658b62f-7129-4da9-8b34-07e460297fd2", "74cc0a89-667d-40b0-bb62-9e0f0a34af85", "def1c3ec-48fb-4786-81ac-23d724976d6a", "ef4200eb-7a67-4f13-996f-212d08e9d221", "8b5ffa73-eeaf-47b6-a29c-d572cdd7ea42", "733503e8-6e1d-4351-8991-db2d630c56dd", "4c431cfa-3853-44e9-bb21-1c1245c36752", "6397dd93-c879-49f4-bf31-8d8576c3c3db", "4b60359c-a455-4130-8861-a5fd25955fbd", "407a718c-48b7-459d-a287-028f6f1899a8", "949714ca-56c3-48d7-ba3a-5337c63a519a", "52e45947-6718-421f-bded-d83055e36d97", "2b93c396-2f42-4bb5-bff3-81fdbbe6546c", "b5fe8ee1-25b3-4515-8dc5-f4f73f5507ab", "99c94d67-61af-42cc-8e2e-fae32d3ba3c6", "e1ad92ac-3aeb-4719-9160-10c1dab00563", "9a9dbf37-a9b7-499d-9d97-815d3de50983", "5aa4b107-4757-47a2-a11e-9605c5f3da1f", "6faa1076-7c26-4e9e-a75c-a29c27698bbf", "7cb186a8-5a82-43af-a85a-3d7c9fb498cf", "c6f75010-c249-4c13-b50b-072cee603031", "3dac663a-04cb-4da4-b383-37a12eb6d083", "7422bbd9-800e-4f59-8a85-667a600f69a2"], "metadata": {}}}}